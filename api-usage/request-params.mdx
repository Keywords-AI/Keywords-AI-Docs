---
title: "Request Parameters"
description: "An overview of LLM generation parameters"
---

You can paste the command below into your terminal to run your first API request. Make sure to replace `YOUR_KEYWORDSAI_API_KEY` with your actual Keywords AI API key.

## Example Call

<CodeGroup>

```bash cURL
curl -X POST "https://api.keywordsai.co/api/generate/" \
-H "Content-Type: application/json" \
-H "Authorization: Api-Key {YOUR_KEYWORDSAI_API_KEY}" \
-d '{
  "messages": [
    {
      "role": "user",
      "content": "Hello"
    }
  ],
  "models": ["gpt-3.5-turbo", "gpt-3.5-turbo-16k"],
  "stream": false,
  "max_tokens": 100
}'
```

```python Python
import requests
def demo_call(input,
              model="claude-2" ,
              token="a4EUZEcl.RmrDVwbTI8yOFZNuKwSwYnrdCc03Qn6Z",
              stream=False
              ):
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Api-Key {token}',
    }

    data = {
        'model': model,
        'messages': [{'role': 'user', 'content': input}],
        "stream": stream,
        "models": ["gpt-3.5-turbo", "gpt-3.5-turbo-16k"],
    }

    response = requests.post('https://api.keywordsai.co/api/generate/', headers=headers, json=data, stream=stream)
    return response

messages = "Say 'Hello World'"
print(demo_call(messages).json())
```

```TypeScript TypeScript
// Define the function with TypeScript
fetch('https://api.keywordsai.co/api/generate/', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Authorization': 'Api-Key a4EUZEcl.RmrDVwbTI8yOFZNuKwSwYnrdCc03Qn6Z'
  }
    body: JSON.stringify({
        models: ['gpt-3.5-turbo', 'gpt-3.5-turbo-16k'],
        messages: [{role: 'user', content: "Say 'Hello World'"}]
    })
})
.then(response => response.json())
.then(data => console.log(data));
```

</CodeGroup>

## LLM API Parameters

<ParamField path="model" type="string">
  Specify which model to use. See the list of model
  [here](/integration/supported-models)
</ParamField>

<ParamField path="messages" type="array[dict]" required={true}>
  List of messages to send to the endpoint in the [OpenAI style](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages), each of them following this format:
  ```json
  {
    "role": "user", // Available choices are user, system or assistant
    "content": "Hello?"
  }
  ```

  <strong>Image Processing</strong> If you want to use the image processing feature, you need to use the following format to upload the image
  ```json
{
  "role": "user",
  "content": [
    {
        "type": "text",
        "text": "Whatâ€™s in this image?"
    },
    {
        "type": "image_url",
        "image_url": {
        "url": "https://as1.ftcdn.net/v2/jpg/01/34/53/74/1000_F_134537443_VendrqyXIWyHrZgxdIsfyKUost734JDP.jpg"
        }
    }
  ]
}
  ```
</ParamField>

<ParamField path="models" type="array[string]">
Specify the list of models for the router to choose between. If not specified, <strong>all models</strong> will be used. See the list of models [here](/integration/supported-models)
</ParamField>

<ParamField path="fallback_models" type="array[string]">
Specify the list of backup models (ranked by priority) to respond in case of a failure in the primary model. See the list of models [here](/integration/supported-models)
</ParamField>


<ParamField path="customer_api_keys" type="dict">
You can pass in a dictionary of your customer's API keys for specific models. If the router selects a model that is in the dictionary, it will attempt to use the customer's API key for calling the model before using your [integration API key](/integration/llm-credits) or Keywords AI's default API key.

```json
{
  "gpt-3.5-turbo": "your_customer_api_key",
  "gpt-4": "your_customer_api_key",
}
```
</ParamField>

<ParamField path="suffix" type="string">
  Strings to append to the end of the response.
</ParamField>

<ParamField path="max_tokens" type="integer">
  Maximum number of tokens to generate in the response
</ParamField>

<ParamField path="temperature" type="float" default={1}>
  Controls randomness in the output in the range of 0-2, higher temperature will
  a more random response. See more info on the [LLM parameters page](/get-started/llm-params)
</ParamField>

<ParamField path="n" type="integer" default={1}>
  Specify many completion choices to generate for each prompt.

  <strong>Caveat!</strong> While this can help improve generation quality by picking the optimal choice, this could also lead to more token usage.
</ParamField>

<ParamField path="stream" type="boolean" default={false}>
  Whether to stream back partial progress token by token
</ParamField>

{/* Add this to the concept page */}

<ParamField path="logprobs" type="boolean" default={false}>
  Include the log probabilities on each token being selected.
</ParamField>

<ParamField path="echo" type="boolean">
  Echo back the prompt in addition to the completion
</ParamField>

{/* Add this to the concept page */}

<ParamField path="stop" type="array[string]">
  Stop sequence
</ParamField>

{/* Add this to the concept page */}

<ParamField path="presence_penalty" type="number">
  Specify how much to penalize new tokens based on whether they appear in the text so
  far. Increases the model's likelihood of talking about new topics
</ParamField>

{/* Add this to the concept page */}

<ParamField path="frequency_penalty" type="number">
  Specify how much to penalize new tokens based on their existing frequency in the text
  so far. Decreases the model's likelihood for repeating the same line verbatim
</ParamField>

{/* Add this to the concept page */}

<ParamField path="logit_bias" type="dict">
  Used to modify the probability of tokens appearing in the response
</ParamField>

<ParamField path="tools" type="array[dict]">
  A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide an array of functions the model may generate JSON inputs for.
  ```json
  {
      "type": "function",
      "function": {
        "name": "get_current_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "The city and state, e.g. San Francisco, CA"
            },
            "unit": {
              "type": "string",
              "enum": ["celsius", "fahrenheit"]
            }
          },
          "required": ["location"]
        }
      }
    }
  ```
</ParamField>

<ParamField path="tool_choice" type="dict">
  Manually picking the choice of tool for the model to use. This will force the model to make a function call every time a function is passed in.
  ```json
  {
    "type": "function",
    "function": {"name": "name_of_the_function"},
  }
  ```
</ParamField>


## Extra Parameters for Monitoring

<ParamField path="request_breakdown" type="boolean" default={false}>
Adding this returns the summarization of the response in the response body. If streaming is on, the metrics will be streamed as the last chunk

<strong>Regular Response</strong>
```json
{
  "id": "chatcmpl-7476cf3f-fcc9-4902-a548-a12489856d8a",
  //... main part of the response body ...
  "request_breakdown": {
    "prompt_tokens": 6,
    "completion_tokens": 9,
    "cost": 4.8e-5,
    "prompt_messages": [
      {
        "role": "user",
        "content": "How are you doing today?"
      }
    ],
    "completion_message": {
      "content": " I'm doing well, thanks for asking!",
      "role": "assistant"
    },
    "model": "claude-2",
    "cached": false,
    "timestamp": "2024-02-20T01:23:39.329729Z",
    "status_code": 200,
    "stream": false,
    "latency": 1.8415491580963135,
    "scores": {},
    "category": "Questions",
    "metadata": {},
    "routing_time": 0.18612787732854486,
    "full_request": {
      "messages": [
        {
          "role": "user",
          "content": "How are you doing today?"
        }
      ],
      "model": "claude-2",
      "logprobs": true
    },
    "sentiment_score": 0
  }
}
```

<strong>Streaming Response</strong>

```json
//... other chunks ...
// The following is the last chunk
{
  "id": "request_breakdown",
  "choices": [
    {
      "delta": { "content": null, "role": "assistant" },
      "finish_reason": "stop",
      "request_breakdown": {
        "prompt_tokens": 6,
        "completion_tokens": 9,
        "cost": 4.8e-5, //  In usd
        "prompt_messages": [
          {
            "role": "user",
            "content": "How are you doing today?"
          }
        ],
        "completion_message": {
          "content": " I'm doing well, thanks for asking!",
          "role": "assistant"
        },
        "model": "claude-2",
        "cached": false,
        "timestamp": "2024-02-20T01:23:39.329729Z",
        "status_code": 200,
        "stream": false,
        "latency": 1.8415491580963135, // in seconds
        "scores": {},
        "category": "Questions",
        "metadata": {},
        "routing_time": 0.18612787732854486, // in seconds
        "full_request": {
          "messages": [
            {
              "role": "user",
              "content": "How are you doing today?"
            }
          ],
          "model": "claude-2",
          "logprobs": true
        },
        "sentiment_score": 0
      },
      "index": 0,
      "message": { "content": null, "role": "assistant" }
    }
  ],
  "created": 1706100589,
  "model": "extra_parameter",
  "object": "chat.completion.chunk",
  "system_fingerprint": null,
  "usage": {}
}
```
</ParamField>

<ParamField path="customer_identifier" type="string">
  Use this as a tag to identify the user associated with the API call.
</ParamField>

<ParamField path="metadata" type="dict">
You can add any key-value pair to this metadata field for your reference, contact team@keywordsai.co if you need extra parameter support for your use case.

Example:

```json
{
  "my_value_key": "my_value"
}
```

</ParamField>

<ParamField path="disable_log" type="boolean" deafault={false}>
  When set to true, only the request and the [performance
  metrics](/performance-monitoring/monitoring-metrics) will be recorded, input
  and output messages will be omitted from the log.
</ParamField>
