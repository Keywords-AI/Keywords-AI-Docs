---
title: Set up an LLM evaluator
description:  In this guide, we will show you how to run an LLM evaluator in the UI.
---
LLM evaluators allows you to evaluate your prompts with the help of LLM. You can evaluate your prompts based on various metrics and see the results in Logs.
<Note>This is a beta feature. Please do let us know if you encounter any issues. Weâ€™ll continuously improve it.</Note>

## Prerequisites
You have already created prompts in the platform. Learn how to create a prompt [here](https://docs.keywordsai.co/features/prompt/create-a-prompt).


## Steps
<Steps>
<Step title="Create a new evaluator">
You can set up an evaluator in [Evaluators](https://platform.keywordsai.co/platform/evaluators). Click the **+ New evaluator** button, and select **LLM**.
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/evaluations/create-evaluator.png"/>
</Frame>

</Step>

<Step title="Configure an LLM evaluator">
Here's a sample evaluator configuration. We'll describe each section in detail below.
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/evaluations/llm-evals/llm-evaluator-overview.png"/>
</Frame>

You need to define a `Slug` for each evaluator. This slug will be used to apply the evaluator in your LLM calls, and will be used to identify the evaluator in the Logs.

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/evaluations/llm-evals/slug.png"/>
</Frame>

<Warning>
The `Slug` is a unique identifier for the evaluator. We suggest you don't change it once you have created the evaluator.
</Warning>

Then, you need to choose a model for the evaluator. The evaluator will use this model to evaluate the LLM outputs. Currently, we only support `gpt-4o` and `gpt-4o-mini` from OpenAI and Azure OpenAI.

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/evaluations/llm-evals/choose-model.png"/>
</Frame>

After that, you need to write a description for the evaluator. This description is for the LLM to understand the task and the expected output. There are 3 variables that you can use in the description:
- `{{llm_output}}`: The output text from the LLM.
- `{{ideal_output}}`: The ideal output text from the LLM. This is optional, you can add it if you want to give the LLM a reference output.

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/evaluations/llm-evals/write-description.png"/>
</Frame>

In the last, you need to define the `Scoring rubric` for the evaluator. This is for the LLM to understand the scoring criteria.

**Passing score** is the minimum score that the LLM output needs to achieve to be considered as a passing response.
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/evaluations/llm-evals/define-scores.png"/>
</Frame>

You're good to go now! Click on the **Save** button to create the evaluator. Let's move on to the next step to see how to run the evaluator in the UI.
</Step>


{/* **Explanation of `extra_params`**

You must pass the `extra_params` based on the metric you choose. For example, if you choose the `Ragas Answer Relevancy` metric, you can find the required parameters are`Question`, `Answer`, and `Contexts`.
<Warning>The `Question` and `Answer` fields are automatically extracted from the conversation log if required by the evaluator, so you don't need to be explicitly provided in extra_params. </Warning>
<img height="200" noZoom src="https://keywordsai-static.s3.amazonaws.com/docs/eval-params.png" /> */}

{/* <Step title="See the result">
The evaluator will automatically run on the LLM call and you will be able to see the results in the side panel of the corresponding log.
</Step> */}
</Steps>

