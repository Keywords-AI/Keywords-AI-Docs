---
title: Overview
icon: "scale-unbalanced"
---

Keywords AI integrates with [Continuous Eval](https://docs.relari.ai/metrics/intro), an open-source package created for granular and holistic evaluation of GenAI application pipelines, to provide a comprehensive evaluation of the LLM-generated results from the Keywords AI API.

You can find the evaluation settings at Keywords AI (top of left nav bar) > [Evaluation](https://platform.keywordsai.co/platform/api/evaluations).
<img src="/images/evaluation/overview.png"/>

## Sampling
Evaluations are expensive. You can specify a percentage of API calls to be randomly selected for evaluation.
<img src="/images/evaluation/sampling.png"/>

The default value is _10%_.

## Evaluation metrics
Keywords AI provides the following categories of evaluation metrics:

### 1. Retrieval metrics:
- **Context precision**: measures information density ([see more](/features/evaluation/retrieval/context-precision)).

### 2. Text generation metrics:
- **Faithfulness**: measures how grounded is the answer on the retrieved contexts ([see more](/features/evaluation/generation/faithfulness)).
- **Fleschâ€“Kincaid readability**: measures the readability of the generated text ([see more](/features/evaluation/generation/flesch-kincaid)).
- **Relevance**: measures the relevance of the generated text to the query ([see more](/features/evaluation/generation/answer-relevance)).

### 3. Other metrics:
- **Sentiment**: measures the sentiment of the generated text ([see more](/features/evaluation/other/sentiment)).
{/* - **Topic**: classify the topic of the generated text ([see more](/features/evaluation/other/topic)). */}


## Run evaluation mannually

You can run the evaluation manually by clicking the `Run` button in the side panel of the Requests page.
<img src="/images/evaluation/manually-evaluation.png"/>