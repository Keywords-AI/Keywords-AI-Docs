---
title: Overview
description: "A guide to benchmark LLM performance with evals."
---

## What is evaluations?
Evaluations help you assess the performance of your prompts. You can create custom evals to measure different dimensions of output quality.

## Why use evaluations?
- To measure the quality of a prompt or a model.
- To find the best prompt or model for a specific task.
- To optimize your prompts and models.

## Quickstart

<CardGroup cols={2}>
<Card title="Create an LLM-as-judge evaluator" href="../evaluation/llm-evals/llm-evaluator">

</Card>
<Card title="Create a human evaluator" href="../evaluation/human-evals/human-evals-setup">
</Card>

<Card title="Run experiments with testsets" href="../evaluation/experiments/overview">
</Card>
</CardGroup>

