---
title: Overview
---

Keywords AI integrates with [Continuous Eval](https://docs.relari.ai/v0.3/getting-started/introduction), an open-source package created for granular and holistic evaluation of GenAI application pipelines, to provide a comprehensive evaluation of the LLM-generated results from the Keywords AI API.

You can find the evaluation settings at Keywords AI (top of left nav bar) > [Evaluation](https://platform.keywordsai.co/platform/api/evaluations).
<img src="/images/evaluation/overview.png"/>

## Sampling
Evaluations are expensive. You can specify a percentage of API calls to be randomly selected for evaluation.
<img src="/images/evaluation/sampling.png"/>

The default value is 10%.

## Evaluation Metrics
Keywords AI provides the following categories of evaluation metrics:

### 1. Retrieval Metrics:
- **Context Precision**: measures information density ([see more](/features/evaluation/retrieval/context-precision)).

### 2. Text Generation Metrics:
- **Faithfulness**: measures how grounded is the answer on the retrieved contexts ([see more](/features/evaluation/generation/faithfulness)).
- **Fleschâ€“Kincaid Readability**: measures the readability of the generated text ([see more](/features/evaluation/generation/flesch-kincaid)).
- **Relevance**: measures the relevance of the generated text to the query ([see more](/features/evaluation/generation/relevance)).

### 3. Other Metrics:
- **Sentiment**: measures the sentiment of the generated text ([see more](/features/evaluation/other/sentiment)).
- **Topic**: classify the topic of the generated text ([see more](/features/evaluation/other/topic)).