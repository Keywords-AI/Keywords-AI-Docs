---
title: "Run prompt experiments"
description: "A spreadsheet-like interface built for multi-config prompt testing"
---

A spreadsheet-style editor for running prompts and models across multiple test cases. Import testsets to easily test, evaluate, and optimize your LLM outputs.
<Frame>
<video
  controls
  className="w-full aspect-video"
  src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/evaluations/llm-evals/lab%2Beval.mp4"
></video>
</Frame>
## Prerequisites
- You have already created at least one prompt in Keywords AI. Learn how to create a prompt [here](/features/prompt/create-a-prompt).
- You have added variables to your prompt. Learn how to add variables to your prompt [here](/features/prompt/create-a-prompt#4-variables-in-the-prompt).

## Steps
<Steps>

<Step title="Create a testset (optional)">
Go to Testsets and create a new testset, you can either import a CSV file or create a blank testset. You can check [Testsets](/features/prompt/testsets) to know more about testsets.
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/evaluations/llm-evals/experiments/testset-create.png" />
</Frame>
</Step>

<Step title="Create an experiment">
Go to Experiments and create a new experiment.
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/evaluations/llm-evals/experiments/experiment-create.png" />
</Frame>

Then you should select the prompt and the versions you want to test. 
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/evaluations/llm-evals/experiments/experiment-select-prompt.png" />
</Frame>
</Step>

<Step title="Add test cases">
Then you should add test cases for your experiment. You can either add test cases manually or import a testset from Testsets.
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/evaluations/llm-evals/experiments/add-testcases.png" />
</Frame>
</Step>

<Step title="Run the experiment">
Now you can run the experiment. You can run a single cell by clicking the `Run` button in the each cell, or run all the cells by clicking the `Run all` button.
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/evaluations/llm-evals/experiments/run-experiments.png" />
</Frame>
</Step>

<Step title="Run evaluations for outputs">
After the experiment is finished, you can run evaluations for the outputs. You can check out [Run evaluations in the UI](/features/evaluation/llm-evals/run-evaluator-ui) to learn how to run evaluations.
<Frame>
<video
  controls
  className="w-full aspect-video"
  src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/evaluations/llm-evals/experiments/evals-result.mp4"
></video>
</Frame>
</Step>

</Steps>
{/* 
## What's next?
After you have evaluated the prompts, you can save the results to the [Logs page](/features/generation/logs) for further analysis. If you're satisfied with the results, you can deploy the prompt to your application. Check out the [Prompts page](/features/prompt/prompt-management) for more information.  */}