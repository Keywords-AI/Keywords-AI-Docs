---
title: "Vercel AI SDK"
description: "Learn how to integrate KeywordsAI tracing with Vercel AI SDK to monitor and analyze your AI application performance. Step-by-step guide for setting up environment variables and creating traced workflows."
---

## Prerequisites
- Vercel AI SDK
- [KeywordsAI API key](https://platform.keywordsai.co/platform/api/api-keys)

## Steps

<Steps>
<Step title="Install Keywords AI SDK"> 
  ```bash
npm install @keywordsai/tracing
  ```
</Step>
<Step title="Add an instrumentation file to the root of your project">
You should add an instrumentation file to the root of your project. For example, `instrumentation.ts`. Same level as `.env` or `next.config.js` file.

Then add the `KeywordsAIExporter` as the trace exporter

```typescript root-of-project/instrumentation.js
import { KeywordsAIExporter } from "@keywordsai/tracing";
import { registerOTel } from "@vercel/otel";
// This file is a special Next.js file that is loaded on startup
export async function register() {
  try {
    // Only load the instrumentation in the Node.js environment (not in edge runtime or client)
    registerOTel({
      serviceName: "keywords-ai-example",
      traceExporter: new KeywordsAIExporter({
        //   baseUrl: "https://api.keywords.co", # This will be the default base URL. enterprise customer should change it
        apiKey: process.env.KEYWORDSAI_API_KEY_TEST,
      }),
    });
  } catch (error) {
    console.error("Failed to load instrumentation:", error);
  }
}
```
</Step>
<Step title="(optional) Configure next.config.js">
**If you are using Next.js, please do the following:**
1. Add node-loader package
```bash
npm install node-loader
```
2. Add the following to your `next.config.js` file:
```typescript next.config.js
const nextConfig = {
webpack: (config, { isServer }) => {
  config.module.rules.push({
    test: /\.node$/,
    loader: "node-loader",
  });
  if (isServer) {
    config.ignoreWarnings = [{ module: /opentelemetry/ }];
  }
  // In this block, we will ignore/browserify some node dependencies
     config.resolve.alias = {
      ...config.resolve.alias,
      zlib: require.resolve("browserify-zlib"),
    };
    config.resolve.fallback = {
      ...config.resolve.fallback,
      tls: false,
      fs: false,
      http: false,
      https: false,
      http2: false,
      net: false,
      dns: false,
      os: false,
      path: false,
      stream: false,
    };
  // end of ignoring/browserify node depenencies
  return config;
},
};
```
</Step>
<Step title="Send traces to KeywordsAI">
Then, to trace a specific function, simply enable tracing. here is an example of it working on the streamText function:
```typescript
import { openai } from "@ai-sdk/openai";
import { streamText } from "ai";

const result = await streamText({
  model: openai("gpt-4o"),
  messages: convertToCoreMessages(messages),
  // Add this block here
  experimental_telemetry: {
    isEnabled: true,
  }
  // End of add this block
});
```
</Step> 
</Steps>


{/* ## Set up environment variables
Create a `.env` file in your project root and add the following variables:
```
KEYWORDSAI_BASE_URL=https://api.keywordsai.co/api
KEYWORDSAI_API_KEY=YOUR_KEYWORDSAI_API_KEY
```


## Create a task with Vercel AI SDK
In this example, we create 2 tasks: `getUserInput` and `getLLMResponse` and annotate them with KeywordsAI tracing.
```typescript Vercel AI SDK {3, 5-10,16-17,33-34}
import { CoreMessage, streamText } from "ai";
import { createOpenAI } from "@ai-sdk/openai";
import { KeywordsAITelemetry } from "@keywordsai/tracing";

const kTl = new KeywordsAITelemetry({
  apiKey: process.env.KEYWORDSAI_API_KEY || "",
  baseUrl: process.env.KEYWORDSAI_BASE_URL || "",
  disableBatch: true,
});

const client = createOpenAI({
  compatibility: "strict",
});

function getUserInput(userName: string) {
  return kTl.withTask(
    { name: "getUserInput" },
    async (userName: string): Promise<CoreMessage> => {
      return {
        role: "user",
        content: "Hello, my name is:  " + userName,
      };
    },
    userName
  );
}

async function getLLMResponse(
  messages: CoreMessage[],
  model: string,
  temperature: number
) {
  const proxyTextStream = await kTl.withTask(
    { name: "getLLMResponse" },
    async (kwargs: { messages: CoreMessage[]; model: string; temperature: number }) => {
      const { textStream: proxyTextStream } = streamText({
        model: client.chat(kwargs.model),
        messages: kwargs.messages,
        temperature: kwargs.temperature,
      });
      return proxyTextStream;
    },
    {
      messages: messages,
      model: model,
      temperature: temperature,
    }
  );
  return proxyTextStream;
}
```

## Combine tasks into a workflow
In this example, we create a workflow `vercelWithTracing` that combines the `getUserInput` and `getLLMResponse` tasks.
<CodeGroup>
```typescript Vercel AI SDK {3, 5-10,25-26}
import { CoreMessage, streamText } from "ai";
import { createOpenAI } from "@ai-sdk/openai";
import { KeywordsAITelemetry } from "@keywordsai/tracing";

const kTl = new KeywordsAITelemetry({
  apiKey: process.env.KEYWORDSAI_API_KEY || "",
  baseUrl: process.env.KEYWORDSAI_BASE_URL || "",
  disableBatch: true, // Send a trace immediately when it's ready, no batching
});

function getUserInput(userName: string) {
  // ...
}

function getLLMResponse(messages: CoreMessage[], model: string, temperature: number) {
  // ...
}

  async function main() {
  // Three arguments for the withWorkflow decorator:
  // 1. DecoratorConfig
  // 2. Function to decorate
  // 3. Arbitrary arguments for the function

  return kTl.withWorkflow(
    { name: "vercelWithTracing" },
    async (kwargs) => {
      try {
        const textPieces: string[] = [];
        const proxyTextStream = await getLLMResponse(
          [await getUserInput("John")],
          kwargs.model,
          kwargs.temperature
        );
        for await (const textPart of proxyTextStream) {
          textPieces.push(textPart);
        }
        return textPieces.join("");
      } catch (error) {
        console.error("Error:", error);
      }
    },
    {
      model: "gpt-3.5-turbo",
      temperature: 0.5,
    }
  );
}

main();
```
</CodeGroup> */}


