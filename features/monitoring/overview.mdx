---
title: "Overview"
description: "Log your LLM requests and responses asynchronously"
og:title: "AI observability"
---

<Tabs>

<Tab title="Metrics">
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/observability/metrics/dashboard.png" />
</Frame>
</Tab>

<Tab title="Logging">
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/observability/overview/logs.jpg" />
</Frame>
</Tab>

{/* <Tab title="Threads">
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/observability/threads/threads-list.png" />
</Frame>
</Tab> */}

<Tab title="Agent tracing">
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/observability/overview/traces.png" />
</Frame>
</Tab>

<Tab title="User analytics">
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/observability/overview/users.png" />
</Frame>
</Tab>

</Tabs>

## Why you need LLM observability?
- **Performance monitoring**: Track response times, token usage, and model performance to ensure your AI systems are working optimally.
- **Cost management**: Gain visibility into model usage patterns, identify expensive prompts, and optimize spending across different LLM providers.
- **Quality assurance**: Detect hallucinations, accuracy issues, and unexpected outputs before they impact your users.
- **Debugging**: Quickly identify and troubleshoot issues by examining the complete AI session.
- **Usage analytics**: Understand how users interact with your AI features and which prompts generate the most value.

Without proper observability, LLM-powered applications become black boxes - expensive to run, difficult to debug, and impossible to systematically improve.

## Getting started
<CardGroup cols={1}>

<Card title="Logging" href="/features/monitoring/get-started/logging-api">
Log your LLM requests and responses asynchronously
</Card>

<Card title="LLM proxy" href="/features/monitoring/get-started/proxy-api">
Call 250+ LLMs using the same input/output format
</Card>

<Card title="Tracing" href="/features/monitoring/get-started/tracing-api">
Trace and debug AI agent workflows
</Card>
</CardGroup>   

{/* ## Why you need LLM observability?
LLM observability is the comprehensive process of monitoring, evaluating, and gaining insights into the performance and activities of Large Language Models in real-time. LLM observability is crucial for several reasons:

1. **Ensuring Accuracy and Relevance**: LLMs can produce hallucinations, so observability helps detect and fix these issues.
2. **Maintaining Performance**: Tracking response times, throughput, and error rates ensures optimal LLM performance.
3. **Enhancing Reliability**: Monitoring helps prevent and quickly resolve downtime from provider outages, rate limits, or delayed alerts.
4. **Optimizing Costs**: Monitoring identifies cost-effective models and leverages caching to reduce expenses.

Keywords AI provides an **Async Logging API** that allows you to log your LLM requests and responses asynchronously, which offers complete observability of your LLM applications and won't disrupt your application's performance.
 */}

{/* ## Benefits of async logging:
- Monitor your LLM performance with **0 latency impact**.
- Operates outside the critical path of your application, ensuring no disruptions.
- Gain comprehensive observability of your LLM applications. */}
