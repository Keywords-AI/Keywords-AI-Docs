---
title: "Overview"
description: "Log your LLM requests and responses asynchronously"
og:title: "AI observability"
---

## Why you need LLM observability?
LLM observability is the comprehensive process of monitoring, evaluating, and gaining insights into the performance and activities of Large Language Models in real-time. LLM observability is crucial for several reasons:

1. **Ensuring Accuracy and Relevance**: LLMs can produce hallucinations, so observability helps detect and fix these issues.
2. **Maintaining Performance**: Tracking response times, throughput, and error rates ensures optimal LLM performance.
3. **Enhancing Reliability**: Monitoring helps prevent and quickly resolve downtime from provider outages, rate limits, or delayed alerts.
4. **Optimizing Costs**: Monitoring identifies cost-effective models and leverages caching to reduce expenses.

Keywords AI provides an **Async Logging API** that allows you to log your LLM requests and responses asynchronously, which offers complete observability of your LLM applications and won't disrupt your application's performance.


## Benefits of async logging:
- Monitor your LLM performance with **0 latency impact**.
- Operates outside the critical path of your application, ensuring no disruptions.
- Gain comprehensive observability of your LLM applications.
