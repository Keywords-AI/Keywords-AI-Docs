---
title: "Overview"
description: "A guide to log, monitor, and improve LLM outputs."
---

## Features
<Tabs>

<Tab title="LLM logs">
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/observability/overview/logs.jpg" />
</Frame>
</Tab>

<Tab title="Filters">
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/observability/logs/filters.png" />
</Frame>
</Tab>

<Tab title="Full-text search">
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/observability/logs/full-text-search.png" />
</Frame>
</Tab>

</Tabs>

## What is an LLM log?

An LLM log is a record of an LLM request. It includes the prompt, the response, and the metadata associated with the request. 

In Keywords AI, you can see every LLM log's metrics like `Messages`, `Model`, `Provider`, `User`, `API key`, `Prompt`, `Response`, `Cost`, `Duration`, `Status`, and `Timestamp`.

## You will love it when you want to:
- See the prompt and response of an LLM log.
- Debug an LLM error.
- Target a specific LLM log.
- See metadata of each LLM log.

## Quick start
To enable logging, you can use the Logging API,Tracing API, or the LLM proxy to log each LLM request.

<CardGroup>
<Card title="Logging API" href="/features/monitoring/get-started/logging-api">

</Card>

<Card title="Tracing API" href="/features/monitoring/get-started/tracing-api">

</Card>

<Card title="LLM proxy" href="/features/monitoring/get-started/llm-proxy">

</Card>
</CardGroup>
