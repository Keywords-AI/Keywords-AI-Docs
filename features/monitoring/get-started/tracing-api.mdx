---
title: "Set up via Tracing SDK"
description: "You can use Keywords AI Traces to trace your LLM requests and responses."
og:title: 'LLM tracing'
---

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/observability/traces/traces-overview.png" />
</Frame>

## What is Tracing?
LLM tracing is a feature that allows you to trace your LLM requests and responses. It is a way to track the workflow of LLM calls and tools of your AI application.

## How to set up Tracing?
<Tabs>
<Tab title="Python">

<Steps>
<Step title="Install the SDK">
Install the package using your preferred package manager:
<CodeGroup>
```bash pip
pip install keywordsai-tracing
```
```bash poetry
poetry add keywordsai-tracing
```
</CodeGroup>
</Step>

<Step title="Set up Environment Variables">
Get your API key from the [API Keys page](https://platform.keywordsai.co/platform/api/api-keys) in Settings, then configure it in your environment:

```python .env
KEYWORDSAI_BASE_URL="https://api.keywordsai.co/api"
KEYWORDSAI_API_KEY="YOUR_KEYWORDSAI_API_KEY"
```
<Tip> `KEYWORDSAI` is **one word** in the environmental variabl name </Tip>

</Step>

<Step title="Annotate your workflows">
Use the `@workflow` and `@task` decorators to instrument your code:
```python Python {6, 8}
from keywordsai_tracing.decorators import workflow, task
from keywordsai_tracing.main import KeywordsAITelemetry

k_tl = KeywordsAITelemetry()

@workflow(name="my_workflow")
def my_workflow():
    @task(name="my_task")
    def my_task():
        pass
    my_task()
```
</Step>

<Step title="A full example with LLM calls">
In this example, you will see how to implement a workflow that includes LLM calls. We use OpenAI SDK as an example.

```python main.py {2-3, 5, 8}
from openai import OpenAI
from keywordsai_tracing.decorators import workflow, task
from keywordsai_tracing.main import KeywordsAITelemetry

k_tl = KeywordsAITelemetry()
client = OpenAI()

@workflow(name="create_joke")
def create_joke():
    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": "Tell me a joke about opentelemetry"}],
        temperature=0.5,
        max_tokens=100,
        frequency_penalty=0.5,
        presence_penalty=0.5,
    )
    return completion.choices[0].message.content
```
</Step>

</Steps>
</Tab>

<Tab title="JS/TS">
<Steps>
<Step title="Install the SDK">
Install the package using your preferred package manager:
```bash 
npm install @keywordsai/tracing
# or yarn

yarn add @keywordsai/tracing
```
</Step>
<Step title="Set up Environment Variables">
Get your API key from the [API Keys page](https://platform.keywordsai.co/platform/api/api-keys) in Settings, then configure it in your environment:

```bash .env
KEYWORDSAI_BASE_URL="https://api.keywordsai.co/api"
KEYWORDSAI_API_KEY="YOUR_KEYWORDSAI_API_KEY"
```
</Step>
<Step title="Create a simple task">
```typescript server.ts {1, 6-10, 16-17}
import { KeywordsAITelemetry } from '@keywordsai/tracing';
import OpenAI from 'openai';

// Initialize clients
// Make sure to set these environment variables or pass them directly
const keywordsAi = new KeywordsAITelemetry({
    apiKey: process.env.KEYWORDSAI_API_KEY || "",
    appName: 'test-app',
    disableBatch: true  // For testing, disable batching
});

const openai = new OpenAI();

// This demonstrates a simple LLM call wrapped in a task
async function createJoke() {
    return await keywordsAi.withTask(
        { name: 'joke_creation' },
        async () => {
            const completion = await openai.chat.completions.create({
                messages: [{ role: 'user', content: 'Tell me a joke about TypeScript' }],
                model: 'gpt-4o-mini',
                temperature: 0.7
            });
            return completion.choices[0].message.content;
        }
    );
}
```
</Step>

<Step title="Create a workflow combining tasks">
In this example, we create a workflow `pirate_joke_workflow` that combines the `createJoke` task with a `translateJoke` task.
```typescript server.ts {2-3, 5-6}
async function jokeWorkflow() {
    return await keywordsAi.withWorkflow(
        { name: 'pirate_joke_workflow' },
        async () => {
            const joke = await createJoke();
            const pirateJoke = await translateJoke(joke);
            return pirateJoke;
        }
    );
}
```
</Step>

</Steps>
</Tab>

</Tabs>

You can now see your traces in the [Traces](https://platform.keywordsai.co/platform/traces) and go to [Logs](https://platform.keywordsai.co/platform/requests) to see the details of your LLM calls.
