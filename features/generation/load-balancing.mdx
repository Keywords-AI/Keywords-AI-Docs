---
title: "Load balancing"
description: "Balance the load of your LLM requests between different models."
icon: "chart-network"
---

Load balancing is a feature that allows you to balance the request load across different models or deployments. You could specify weights for each model/deployment based on their rate limit and your preference.

_See all supported params [here](/api-endpoints/proxy-endpoints/chat-completions)._


## Load balancing between deployments
You could go to the platform and add multiple deployments for the same provider. You could specify the load balancing weights for each deployment, which could be helpful when you want to enhance rate limits for a single provider.

<video
  controls
  className="w-full aspect-video"
  src="/images/api-features/webhooks/loadbalancing.mp4"
></video>

## Load balancing between models
You could specify the load balancing weights for different models. This is useful when you want to balance the load between different models from different providers. 

You could also add your own credentials to use your credits. See how to add your own credentials [here](/integration/own-api-keys).

For the example below, the percentage of requests sent to `claude-3-5-sonnet-20240620` and `azure/gpt-35-turbo` is 50% each.

**This Parameter will overwrite the `model` parameter!**

```json
{
    // ...other parameters...
    "loadbalance_models": [
        {
            "model": "claude-3-5-sonnet-20240620",
            "weight": 34,
            "credentials": { // Your own Anthropic API key, optional for team plan and above
                "api_key": "Your own Anthropic API key"
            }
        },
        {
            "model": "azure/gpt-35-turbo",
            "weight": 34,
            "credentials": { // Your own Azure credentials, optional for team plan and above
                "api_base": "Your own Azure api_base",
                "api_version": "Your own Azure api_version",
                "api_key": "Your own Azure api_key"
            }
        }
    ]
}
```
<ParamField path="model" type="string" required>
  Specify which model to balance load. See the list of models [here](/integration/supported-models). 
</ParamField>
<ParamField path="weight" type="integer" required>
    Specify the weight of the model (has to be a positive integer). The higher the weight, the more requests will be sent to the model.
</ParamField>
<ParamField path="credentials" type="list" >
This is required for all free plan users. For team plan and above, this is optional. <br/>
See how to add your own credentials [here](/integration/providers/providers-overview).
</ParamField>

You could also set up fallback models to avoid downtime. If the model you specified in the `model` field is not available, it will fall back to the list of models you specified in the `fallback` field. Check out the [Fallbacks](/features/generation/fallbacks) section for more information.

{/* 
## How load balancing works
Under the hood, here is the pseudo-code of how the load balancing works:

```python
# Received model in the request
model_name = "requested_model_from_request"
# Load the model's param and its respective providers
model: dict = keywords_ai_model_data[model_name] # A slice from our data

fallback_models = model["load_balancing_fallbacks"]

balancing_between = []
for fallback in fallback_models:
    fallback_param = pack_fallback_list_object(fallback)
    """
    A fallback_param looks something like this:
    {
        "model_name": "model_that_needs_a_fallback",
        "fallback_name": "model_to_fallback_to",
        "rate_limit": 1000,
        "latency": This is live data from the provider based on daily ping
    }
    """
    balancing_between.append(fallback_param)

# ...Omit code until generation...
# Randomly pick a provider based on the latency and rate_limit distribution
model = pick_model_to_use(balancing_between)
keywords_generation(module, **other_kwargs)

```


distribution = [rate_limit_1, rate_limit_2, rate_limit_3] / sum_of_rate_limits <br/>
weights = add_latency_weight_to_distribution(distribution, [latency_1, latency_2, latency_3])<br/>
model = random.choices(balancing_between, weights=weights) */}