---
title: "Compare & debug prompts"
description: "A spreadsheet-like interface built for multi-config prompt testing"
---
<Note> This is a **beta** feature. Please do let us know if you encounter any issues. We'll continuously improve it.</Note>
A spreadsheet-style editor for running prompts and models across multiple test cases. Import testsets to easily test, evaluate, and optimize your LLM outputs.
<Frame>
<video
  controls
  className="w-full aspect-video"
  src="https://keywordsai-static.s3.amazonaws.com/landing/changelog/lab.mp4"
></video>
</Frame>
## Guide

<Steps>
<Step title="Create a prompt">
You should first create a new prompt on the [Prompts page](/features/prompt/prompt-management) to use in the lab.
</Step>

<Step title="Create a testset (optional)">
If you want to run prompts with various dynamic test cases, you can create a new testset on the [Testsets page](/features/prompt/testsets).
</Step>

<Step title="Import prompt and testset">
Now, choose the prompt and the versions you want to test. You can also import a testset if you have one.
<Frame>
<img height="200" noZoom src="https://keywordsai-static.s3.amazonaws.com/docs/lab-import.gif" />
</Frame>
</Step>

<Step title="Evaluate prompts">
Variables in prompts will be detected and displayed in the lab. You can edit the values and run the prompts to evaluate the outputs.
<Frame>
<img height="200" noZoom src="https://keywordsai-static.s3.amazonaws.com/docs/lab-run.gif" />
</Frame>

</Step>
</Steps>

## What's next?
After you have evaluated the prompts, you can save the results to the [Logs page](/features/generation/logs) for further analysis. If you're satisfied with the results, you can deploy the prompt to your application. Check out the [Prompts page](/features/prompt/prompt-management) for more information. 