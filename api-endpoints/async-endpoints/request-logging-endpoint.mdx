---
title: Request Logging Endpoint
api: POST https://api.keywordsai.co/api/request-logs/create/
---

The Request Logging Endpoint is an endpoint that allows you to directly log an LLM inference to Keywords AI, instead of using Keywords AI as a proxy with the [chat completion endpoint](/api-endpoints/proxy-endpoints/chat-completions) and auto-generate the logs.

<ResponseField name="prompt_tokens" type="integer">
Number of tokens in the prompt. Default is 0.
</ResponseField>

<ResponseField name="completion_tokens" type="integer">
Number of tokens in the completion. Default is 0.
</ResponseField>

<ResponseField name="cost" type="float">
Cost of the inference in US dollars. Default is 0.
</ResponseField>

<ResponseField name="prompt_messages" type="array">
Array of prompt messages. Default is an empty list.
</ResponseField>

<ResponseField name="completion_message" type="dict">
Completion message in JSON format. Default is an empty dictionary.
</ResponseField>

<ResponseField name="latency" type="float">
Total generation time. Time from receiving the first token to receiving the last token in seconds. **Do not** confuse this with `time_to_first_token`. Default is 0.
</ResponseField>

<ResponseField name="model" type="string">
Model used for the LLM inference. Default is an empty string.
</ResponseField>

<ResponseField name="failed" type="boolean">
Whether the LLM inference failed. Default is false.
</ResponseField>

<ResponseField name="error_message" type="text">
Error message if the LLM inference failed. Default is an empty string.
</ResponseField>

<ResponseField name="category" type="string">
Category of the log. Default is an empty string.
</ResponseField>

<ResponseField name="time_to_first_token" type="float">
Time from sending the request to LLM for inference to receiving the first token in seconds. Default is 0.
</ResponseField>

<ResponseField name="metadata" type="dict">
Additional variables for this log. Default is an empty dictionary.

```json
{
  "key": "value"
}
```
</ResponseField>

<ResponseField name="stream" type="boolean">
Whether the LLM inference was streamed. Default is false.
</ResponseField>

<ResponseField name="status_code" type="integer">
The status code of the LLM inference. Default is 200 (ok).
</ResponseField>

<ResponseField name="customer_identifier" type="string">
An identifier for the customer that invoked this LLM inference, helps with visualizing user activities. Default is an empty string.
</ResponseField>

<ResponseField name="cached_response" type="integer">
Whether the response was cached. Default is 0.
</ResponseField>