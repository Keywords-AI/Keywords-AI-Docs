---
title: Chat Completions
api: POST https://api.keywordsai.co/api/chat/completions
---

## OpenAI Parameters 
To use [Keywords AI parameters](/api-endpoints/proxy-endpoints/chat-completions#keywords-ai-parameters), you can pass them in the `extra_body` parameter.

<RequestExample>

```Python Python
from openai import OpenAI

client = OpenAI(
    base_url="https://api.keywordsai.co/api/",
    api_key="YOUR_KEYWORDSAI_API_KEY",
)

response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role":"user", "content":"Tell me a long story"}],
    stream=True,
    extra_body={"customer_identifier": "customer_11"}
)
```

```TypeScript TypeScript
import OpenAI from "openai";

const openai = new OpenAI({
  baseURL: "https://api.keywordsai.co/api/",
  apiKey: "YOUR_KEYWORDSAI_API_KEY",
});

const chatCompletion = await openai.chat.completions.create({
  model: "gpt-3.5-turbo",
  messages: [{"role": "user", "content": "Hello"}],
});

```
```Python Functions.py
from openai import OpenAI
client = OpenAI(
    base_url="https://api.keywordsai.co/api/",
    api_key="YOUR_KEYWORDSAI_API_KEY",
)

tools = [
  {
    "type": "function",
    "function": {
      "name": "get_current_weather",
      "description": "Get the current weather in a given location",
      "parameters": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city and state, e.g. San Francisco, CA",
          },
          "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
        },
        "required": ["location"],
      },
    }
  }
]
messages = [{"role": "user", "content": "What's the weather like in Boston today?"}]
completion = client.chat.completions.create(
  model="gpt-4o",
  messages=messages,
  tools=tools,
  tool_choice="auto"
)
print(completion)
```

</RequestExample>



{/* ```python Python
import requests
import os

response = keywords_ai_generate(**params)

def  keywords_ai_generate(
                messages,
                api_key=os.getenv("KEYWORDS_AI_API_KEY"),
                url="https://api.keywordsai.co/api/chat/completions",
                **kwargs
                ):
    print("Calling: ",url)
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}',
    }
    data = {
        "messages": messages,
        **kwargs
    }

    response = requests.post(url=url, headers=headers, json=data)
    return response

params ={"messages": [{"role": "user", "content": "Hello"}], 
         "model": "gpt-3.5-turbo", 
         "stream": False, 
         "max_tokens": 100
         }

```

```TypeScript TypeScript
// Define the function with TypeScript
fetch('https://api.keywordsai.co/api/chat/completions', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer {YOUR_KEYWORDSAI_API_KEY}'
  }
    body: JSON.stringify({
        model: 'gpt-3.5-turbo',
        messages: [{role: 'user', content: "Say 'Hello World'"}]
    })
})
.then(response => response.json())
.then(data => console.log(data));
```

```bash cURL
curl -X POST "https://api.keywordsai.co/api/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer {YOUR_KEYWORDSAI_API_KEY}" \
-d '{
  "messages": [
    {
      "role": "user",
      "content": "Hello"
    }
  ],
  "model": "gpt-3.5-turbo",
  "stream": false,
  "max_tokens": 100
}'
``` */}

<ParamField path="messages" type="array" required={true}>
  List of messages to send to the endpoint in the [OpenAI style](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages), each of them following this format:
  ```json
  messages=[
    {"role": "system", // Available choices are user, system or assistant
     "content": "You are a helpful assistant."
    },
    {"role": "user", "content": "Hello!"}
  ]
  ```
  <Accordion title="Properties">
    <ParamField path="type" type="string">
      The type of response format. Options: `json_object` or `text`
    </ParamField>
  </Accordion>

  <strong>Image Processing:</strong> If you want to use the image processing feature, you need to use the following format to upload the image.
  <Accordion title="Example">
```json
{
  "role": "user",
  "content": [
    {
        "type": "text",
        "text": "Whatâ€™s in this image?"
    },
    {
        "type": "image_url",
        "image_url": {
        "url": "https://as1.ftcdn.net/v2/jpg/01/34/53/74/1000_F_134537443_VendrqyXIWyHrZgxdIsfyKUost734JDP.jpg"
        }
    }
  ]
}
  ```
</Accordion>
 
</ParamField>

<ParamField path="model" type="string" required={true}>
  Specify which model to use. See the list of model
  [here](/integration/supported-models)

**Note**: This parameter will be overridden by the `loadbalance_models` parameter.

</ParamField>

<ParamField path="stream" type="boolean" default={false}>
  Whether to stream back partial progress token by token
</ParamField>

<ParamField path="tools" type="array[dict]">
  A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide an array of functions the model may generate JSON inputs for.
  <Accordion title="Example">
  ```json
  {
      "type": "function",
      "function": {
        "name": "get_current_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "The city and state, e.g. San Francisco, CA"
            },
            "unit": {
              "type": "string",
              "enum": ["celsius", "fahrenheit"]
            }
          },
          "required": ["location"]
        }
      }
    }
  ```
    </Accordion>
</ParamField>

<ParamField path="tool_choice" type="dict">
Controls which (if any) tool is called by the model. `none` means the model will not call any tool and instead generates a message. `auto` means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. 

`none` is the default when no tools are present. `auto` is the default if tools are present.

Specifying a particular tool via the code below forces the model to call that tool.
  ```json
  {
    "type": "function",
    "function": {"name": "name_of_the_function"},
  }
  ```
</ParamField>

<ParamField path="frequency_penalty" type="number">
  Specify how much to penalize new tokens based on their existing frequency in
  the text so far. Decreases the model's likelihood of repeating the same line
  verbatim
</ParamField>

<ParamField path="max_tokens" type="number">
  Maximum number of tokens to generate in the response
</ParamField>

<ParamField path="temperature" type="number" default={1}>
  Controls randomness in the output in the range of 0-2, higher temperature will
  a more random response.
</ParamField>

<ParamField path="n" type="number" default={1}>
  How many chat completion choices to generate for each input message.

  <strong>Caveat!</strong> While this can help improve generation quality by picking the optimal choice, this could also lead to more token usage.
</ParamField>

{/* Add this to the concept page */}

<ParamField path="logprobs" type="boolean" default={false}>
  Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.
</ParamField>

<ParamField path="echo" type="boolean">
  Echo back the prompt in addition to the completion
</ParamField>

{/* Add this to the concept page */}

<ParamField path="stop" type="array[string]">
  Stop sequence
</ParamField>

{/* Add this to the concept page */}

<ParamField path="presence_penalty" type="number">
  Specify how much to penalize new tokens based on whether they appear in the
  text so far. Increases the model's likelihood of talking about new topics
</ParamField>

{/* Add this to the concept page */}

{/* Add this to the concept page */}

<ParamField path="logit_bias" type="dict">
  Used to modify the probability of tokens appearing in the response
</ParamField>

<ParamField path="response_format" type="object">
  An object specifying the format that the model must output. Compatible with GPT-4 Turbo and all GPT-3.5 Turbo models newer than gpt-3.5-turbo-1106.

Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.

You must have a "json" as a keyword in the prompt to use this feature.

  <Accordion title="Properties">
    <ParamField path="type" type="string" required={true}>
      The type of response format. options: `json_object`, `text`
    </ParamField>
  </Accordion>
</ParamField>

## Keywords AI Parameters
See how to make a standard Keywords AI API call in the [Quick Start](/get-started/quick-start) guide.
### Generation Parameters

<ParamField path="loadbalance_models" type="array">
  Balance the load of your requests between different models. See the [details of load balancing
  here.](/features/generation/load-balancing)
  <br/>**Note:** This parameter will override the `model` parameter.
  <Accordion title="Example">
  ```json
  {
    // ...other parameters...
    "loadbalance_models": [
        {
            "model": "claude-3-5-sonnet-20240620",
            "weight": 34,
            "credentials": { // Your own Anthropic API key, optional for team plan and above
                "api_key": "Your own Anthropic API key"
            }
        },
        {
            "model": "azure/gpt-35-turbo",
            "weight": 34,
            "credentials": { // Your own Azure credentials, optional for team plan and above
                "api_base": "Your own Azure api_base",
                "api_version": "Your own Azure api_version",
                "api_key": "Your own Azure api_key"
            }
        }
    ]
}
  ```
  </Accordion>
</ParamField>

<ParamField path="fallback_models" type="array">
  Specify the list of backup models (ranked by priority) to respond in case of a
  failure in the primary model. See the [details of fallback models
  here.](/features/generation/fallbacks)
    <Accordion title="Example">
  ```json
{
  // ...other parameters...
    "fallback": [
      "gemini/gemini-pro",
      "mistral/mistral-small",
      "gpt-4o"
    ]
}
  ```
  </Accordion>
</ParamField>

<ParamField path="customer_credentials" type="object">
You can pass in your customer's credentials for [supported providers](/integration/providers/providers-overview) and use their credits when our proxy is calling models from those providers. <br/>See details [here](/integration/own-api-keys)
<Accordion title="Example">
```json
"customer_credentials": {

  "openai": {
    "api_key": "YOUR_OPENAI_API_KEY",
  }
}
```
</Accordion>
</ParamField>

<ParamField path="prompt" type="object">
  The prompt template to use for the completion. You can build and deploy prompts in the [Prompt](/features/prompt/prompt-management).
<AccordionGroup>
  <Accordion title="Properties">
    <ParamField path="prompt_id" type="string" required={true}>
      The ID of the prompt to use. You can find this on the Prompts page.
    </ParamField>
    <ParamField path="variables" type="object">
      The variables to replace in the prompt template.
    </ParamField>
    <ParamField path="version" type="number">
      The version of the prompt to use. If not specified, the latest version will be used.
    </ParamField>
  </Accordion>
    <Accordion title="Example">
```json
{
    "prompt_id": "prompt_id", //paste this from the prompt management page
    "variables": {
       "variable_name": "variable_value"
    },
    "version": 1
}
```
      </Accordion>
</AccordionGroup>

</ParamField>

<ParamField path="disable_log" type="boolean" deafault={false}>
  When set to true, only the request and [performance
  metrics](/performance-monitoring/monitoring-metrics) will be recorded, input
  and output messages will be omitted from the log.
</ParamField>

<ParamField path="models" type="array">
  Specify the list of models for the Keywords AI LLM router to choose between. 
  If not specified,{" "}
  <strong>all models</strong> will be used. See the list of models
  [here](/integration/supported-models)

If only one model is specified, it will be treated as if the `model` parameter is used and the router will not trigger.

When the `model` parameter is used, the router will not trigger, and this parameter behaves as `fallback_models`.

</ParamField>

<ParamField path="exclude_providers" type="array" default={[]}>
  The list of providers to exclude from the LLM router's selection. All models under the provider will be excluded. See the list of providers [here](/integration/providers/providers-overview)

This only excludes providers in the LLM router, if `model` parameter takes precedence over this parameter, and`fallback_models` and [safety net](/features/monitoring/alerts-and-fallback) will still use the excluded models to catch failures.

</ParamField>

<ParamField path="exclude_models" type="array" default={[]}>
The list of models to exclude from the LLM router's selection. See the list of models [here](/integration/supported-models)

This only excludes models in the LLM router, if `model` parameter takes precedence over this parameter, and`fallback_models` and [safety net](/features/monitoring/alerts-and-fallback) will still use the excluded models to catch failures.

</ParamField>

### Observability Parameters

<ParamField path="metadata" type="dict">
You can add any key-value pair to this metadata field for your reference. Contact team@keywordsai.co if you need extra parameter support for your use case.

<Accordion title="Example">
```json
{
  "my_value_key": "my_value"
}
```
</Accordion>
</ParamField>

<ParamField path="customer_identifier" type="string">
  Use this as a tag to identify the user associated with the API call. See the [details of customer identifier here.](/features/generation/customer-identifier)
  <Accordion title="Example">
```json
{
    //...other_params,
    "customer_identifier": "user_123"
}
```
</Accordion>
</ParamField>

<ParamField path="customer_email" type="string">
  This is the email address of the user associated with the API call. You can add your corresponding user's email address to the request, which is helpful for your tracking and monitoring purposes.
    <Accordion title="Example">
```json
{
    //...other_params,
    "customer_email": "hendrix@keywordsai.co"
}
```
</Accordion>
</ParamField>

<ParamField path="request_breakdown" type="boolean" default={false}>
Adding this returns the summarization of the response in the response body. If streaming is on, the metrics will be streamed as the last chunk.

  <Accordion title="Properties">
    <CodeGroup>
    ```json Regular Response
{
  "id": "chatcmpl-7476cf3f-fcc9-4902-a548-a12489856d8a",
  //... main part of the response body ...
  "request_breakdown": {
    "prompt_tokens": 6,
    "completion_tokens": 9,
    "cost": 4.8e-5,
    "prompt_messages": [
      {
        "role": "user",
        "content": "How are you doing today?"
      }
    ],
    "completion_message": {
      "content": " I'm doing well, thanks for asking!",
      "role": "assistant"
    },
    "model": "claude-2",
    "cached": false,
    "timestamp": "2024-02-20T01:23:39.329729Z",
    "status_code": 200,
    "stream": false,
    "latency": 1.8415491580963135,
    "scores": {},
    "category": "Questions",
    "metadata": {},
    "routing_time": 0.18612787732854486,
    "full_request": {
      "messages": [
        {
          "role": "user",
          "content": "How are you doing today?"
        }
      ],
      "model": "claude-2",
      "logprobs": true
    },
    "sentiment_score": 0
  }
}
```
```json Streaming Response
//... other chunks ...
// The following is the last chunk
{
  "id": "request_breakdown",
  "choices": [
    {
      "delta": { "content": null, "role": "assistant" },
      "finish_reason": "stop",
      "request_breakdown": {
        "prompt_tokens": 6,
        "completion_tokens": 9,
        "cost": 4.8e-5, //  In usd
        "prompt_messages": [
          {
            "role": "user",
            "content": "How are you doing today?"
          }
        ],
        "completion_message": {
          "content": " I'm doing well, thanks for asking!",
          "role": "assistant"
        },
        "model": "claude-2",
        "cached": false,
        "timestamp": "2024-02-20T01:23:39.329729Z",
        "status_code": 200,
        "stream": false,
        "latency": 1.8415491580963135, // in seconds
        "scores": {},
        "category": "Questions",
        "metadata": {},
        "routing_time": 0.18612787732854486, // in seconds
        "full_request": {
          "messages": [
            {
              "role": "user",
              "content": "How are you doing today?"
            }
          ],
          "model": "claude-2",
          "logprobs": true
        },
        "sentiment_score": 0
      },
      "index": 0,
      "message": { "content": null, "role": "assistant" }
    }
  ],
  "created": 1706100589,
  "model": "extra_parameter",
  "object": "chat.completion.chunk",
  "system_fingerprint": null,
  "usage": {}
}
```
    </CodeGroup>
  </Accordion>
</ParamField>


## Deprecated Parameters

<ParamField path="customer_api_keys" type="object">
You can pass in a dictionary of your customer's API keys for specific models. If the router selects a model that is in the dictionary, it will attempt to use the customer's API key for calling the model before using your [integration API key](/integration/llm-credits) or Keywords AI's default API key.

```json
{
  "gpt-3.5-turbo": "your_customer_api_key",
  "gpt-4": "your_customer_api_key"
}
```

</ParamField>
