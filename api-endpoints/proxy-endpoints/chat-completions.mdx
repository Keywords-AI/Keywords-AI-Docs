---
title: Chat Completion
api: POST https://api.keywordsai.co/api/chat/completions
---

## Standard Parameters <br/> <sub>Check Keywords AI parameters after this section</sub> 

<br/>

<CodeGroup>

```python Python
import requests
import os

def  keywords_ai_generate( 
                messages, 
                api_key=os.getenv("KEYWORDS_AI_API_KEY"),
                url=os.getenv("KEYWORDS_AI_ENDPOINT") + "api/chat/completions",
                **kwargs
                ):
    print("Calling: ",url)
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}',
    }
    data = {
        "messages": messages,
        **kwargs
    }
        
    response = requests.post(url=url, headers=headers, json=data)
    return response

params ={"messages": [{"role": "user", "content": "Hello"}], "model": "gpt-3.5-turbo", "stream": False, "max_tokens": 100}
response = keywords_ai_generate(**params)
```

```TypeScript TypeScript
// Define the function with TypeScript
fetch('https://api.keywordsai.co/api/chat/completions', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer a4EUZEcl.RmrDVwbTI8yOFZNuKwSwYnrdCc03Qn6Z'
  }
    body: JSON.stringify({
        model: 'gpt-3.5-turbo',
        messages: [{role: 'user', content: "Say 'Hello World'"}]
    })
})
.then(response => response.json())
.then(data => console.log(data));
```

```bash cURL
curl -X POST "https://api.keywordsai.co/api/chat/completions" \
-H "Content-Type: application/json" \
-H "Authorization: Bearer {YOUR_KEYWORDSAI_API_KEY}" \
-d '{
  "messages": [
    {
      "role": "user",
      "content": "Hello"
    }
  ],
  "model": "gpt-3.5-turbo",
  "stream": false,
  "max_tokens": 100
}'
```

</CodeGroup>

<ParamField path="model" type="string">
  Specify which model to use. See the list of model
  [here](/integration/supported-models)

  When this parameter is used, the router will not trigger, and the `models` parameter behaves as `fallback_models`.

</ParamField>

<ParamField path="messages" type="array" required={true}>
  List of messages to send to the endpoint in the [OpenAI style](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages), each of them following this format:
  ```json
  {
    "role": "user", // Available choices are user, system or assistant
    "content": "Hello?"
  }
  ```

  <strong>Image Processing</strong> If you want to use the image processing feature, you need to use the following format to upload the image
  ```json
{
  "role": "user",
  "content": [
    {
        "type": "text",
        "text": "Whatâ€™s in this image?"
    },
    {
        "type": "image_url",
        "image_url": {
        "url": "https://as1.ftcdn.net/v2/jpg/01/34/53/74/1000_F_134537443_VendrqyXIWyHrZgxdIsfyKUost734JDP.jpg"
        }
    }
  ]
}
  ```
</ParamField>

<ParamField path="max_tokens" type="number">
  Maximum number of tokens to generate in the response
</ParamField>

<ParamField path="temperature" type="number" default={1}>
  Controls randomness in the output in the range of 0-2, higher temperature will
  a more random response.
</ParamField>

<ParamField path="n" type="number" default={1}>
  Specify many completion choices to generate for each prompt.

  <strong>Caveat!</strong> While this can help improve generation quality by picking the optimal choice, this could also lead to more token usage.
</ParamField>

<ParamField path="stream" type="boolean" default={false}>
  Whether to stream back partial progress token by token
</ParamField>

{/* Add this to the concept page */}

<ParamField path="logprobs" type="boolean" default={false}>
  Include the log probabilities on each token being selected.
</ParamField>

<ParamField path="echo" type="boolean">
  Echo back the prompt in addition to the completion
</ParamField>

{/* Add this to the concept page */}

<ParamField path="stop" type="array[string]">
  Stop sequence
</ParamField>

{/* Add this to the concept page */}

<ParamField path="presence_penalty" type="number">
  Specify how much to penalize new tokens based on whether they appear in the
  text so far. Increases the model's likelihood of talking about new topics
</ParamField>

{/* Add this to the concept page */}

<ParamField path="frequency_penalty" type="number">
  Specify how much to penalize new tokens based on their existing frequency in
  the text so far. Decreases the model's likelihood of repeating the same line
  verbatim
</ParamField>

{/* Add this to the concept page */}

<ParamField path="logit_bias" type="dict">
  Used to modify the probability of tokens appearing in the response
</ParamField>

<ParamField path="tools" type="array[dict]">
  A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide an array of functions the model may generate JSON inputs for.
  ```json
  {
      "type": "function",
      "function": {
        "name": "get_current_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "The city and state, e.g. San Francisco, CA"
            },
            "unit": {
              "type": "string",
              "enum": ["celsius", "fahrenheit"]
            }
          },
          "required": ["location"]
        }
      }
    }
  ```
</ParamField>

<ParamField path="tool_choice" type="dict">
  Manually picking the choice of tool for the model to use. This will force the model to make a function call every time a function is passed in.
  ```json
  {
    "type": "function",
    "function": {"name": "name_of_the_function"},
  }
  ```
</ParamField>

<ParamField path="response_format" type="object">
  An object specifying the format that the model must output. Compatible with GPT-4 Turbo and all GPT-3.5 Turbo models newer than gpt-3.5-turbo-1106.

  Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.

  You must have a "json" as a keyword in the prompt to use this feature.
  <Expandable title="properties">
    <ParamField path="type" type="string" required={true}>
      The type of response format. options: "json_object", "text"
    </ParamField>
  </Expandable>
</ParamField>

## Keywords AI Parameters

<ParamField path="request_breakdown" type="boolean" default={false}>
Adding this returns the summarization of the response in the response body. If streaming is on, the metrics will be streamed as the last chunk

<strong>Regular Response</strong>
```json
{
  "id": "chatcmpl-7476cf3f-fcc9-4902-a548-a12489856d8a",
  //... main part of the response body ...
  "request_breakdown": {
    "prompt_tokens": 6,
    "completion_tokens": 9,
    "cost": 4.8e-5,
    "prompt_messages": [
      {
        "role": "user",
        "content": "How are you doing today?"
      }
    ],
    "completion_message": {
      "content": " I'm doing well, thanks for asking!",
      "role": "assistant"
    },
    "model": "claude-2",
    "cached": false,
    "timestamp": "2024-02-20T01:23:39.329729Z",
    "status_code": 200,
    "stream": false,
    "latency": 1.8415491580963135,
    "scores": {},
    "category": "Questions",
    "metadata": {},
    "routing_time": 0.18612787732854486,
    "full_request": {
      "messages": [
        {
          "role": "user",
          "content": "How are you doing today?"
        }
      ],
      "model": "claude-2",
      "logprobs": true
    },
    "sentiment_score": 0
  }
}
```

<strong>Streaming Response</strong>

```json
//... other chunks ...
// The following is the last chunk
{
  "id": "request_breakdown",
  "choices": [
    {
      "delta": { "content": null, "role": "assistant" },
      "finish_reason": "stop",
      "request_breakdown": {
        "prompt_tokens": 6,
        "completion_tokens": 9,
        "cost": 4.8e-5, //  In usd
        "prompt_messages": [
          {
            "role": "user",
            "content": "How are you doing today?"
          }
        ],
        "completion_message": {
          "content": " I'm doing well, thanks for asking!",
          "role": "assistant"
        },
        "model": "claude-2",
        "cached": false,
        "timestamp": "2024-02-20T01:23:39.329729Z",
        "status_code": 200,
        "stream": false,
        "latency": 1.8415491580963135, // in seconds
        "scores": {},
        "category": "Questions",
        "metadata": {},
        "routing_time": 0.18612787732854486, // in seconds
        "full_request": {
          "messages": [
            {
              "role": "user",
              "content": "How are you doing today?"
            }
          ],
          "model": "claude-2",
          "logprobs": true
        },
        "sentiment_score": 0
      },
      "index": 0,
      "message": { "content": null, "role": "assistant" }
    }
  ],
  "created": 1706100589,
  "model": "extra_parameter",
  "object": "chat.completion.chunk",
  "system_fingerprint": null,
  "usage": {}
}
```

</ParamField>

<ParamField path="models" type="array">
  Specify the list of models for the router to choose between. If not specified,{" "}
  <strong>all models</strong> will be used. See the list of models
  [here](/integration/supported-models)

  If only one model is specified, it will be treated as if the `model` parameter is used and the router will not trigger.

  When the `model` parameter is used, the router will not trigger, and this parameter behaves as `fallback_models`.
</ParamField>

<ParamField path="fallback_models" type="array">
  Specify the list of backup models (ranked by priority) to respond in case of a
  failure in the primary model. See the list of models
  [here](/integration/supported-models)
</ParamField>

<ParamField path="customer_credentials" type="object">
You can pass in a dictionary of your customer's credentials and deployment variables for [supported providers](/integration/providers/providers-overview) and use their credits when the router is calling models from those providers.
```json
{
  "openai": {
    "api_key": "sk-DEacrWTDndFYhdcLYKF6T3BlbkdfghuyTj4sYL2v1EDhg3iz5",
    "some_vars": "some_values"
  },
  "provider_id": {
    "some_provider_var_names": "some_provider_var_values"
  }
}
```
</ParamField>

<ParamField path="customer_identifier" type="string">
  Use this as a tag to identify the user associated with the API call.
</ParamField>

<ParamField path="metadata" type="dict">
You can add any key-value pair to this metadata field for your reference, contact team@keywordsai.co if you need extra parameter support for your use case.

Example:

```json
{
  "my_value_key": "my_value"
}
```

</ParamField>

<ParamField path="disable_log" type="boolean" deafault={false}>
  When set to true, only the request and the [performance
  metrics](/performance-monitoring/monitoring-metrics) will be recorded, input
  and output messages will be omitted from the log.
</ParamField>

<ParamField path="exclude_models" type="array" default={[]}>
  The list of models to exclude from the router's selection. See the list of models [here](/integration/supported-models)

  This only excludes models in the router, if `model` parameter will take precedence over this parameter, and`fallback_models` and [safety net](/features/monitoring/alerts-and-fallback) will still use the excluded models to catch failures.
</ParamField>

<ParamField path="exclude_providers" type="array" default={[]}>
  The list of providers to exclude from the router's selection. All models under the provider will be excluded. See the list of providers [here](/integration/providers/providers-overview)

  This only excludes models in the router, if `model` parameter will take precedence over this parameter, and`fallback_models` and [safety net](/features/monitoring/alerts-and-fallback) will still use the excluded models to catch failures.
</ParamField>

## Deprecated Parameters

<ParamField path="customer_api_keys" type="object">
You can pass in a dictionary of your customer's API keys for specific models. If the router selects a model that is in the dictionary, it will attempt to use the customer's API key for calling the model before using your [integration API key](/integration/llm-credits) or Keywords AI's default API key.

```json
{
  "gpt-3.5-turbo": "your_customer_api_key",
  "gpt-4": "your_customer_api_key",
}
```
</ParamField>