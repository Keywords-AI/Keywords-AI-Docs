---
title: "Overview"
description: "A guide to view LLM usage metrics and user analytics"
og:title: "AI observability"
---
---

## Why observability matters

**Performance monitoring** tracks response times and model performance to ensure optimal operation.

**Cost management** identifies expensive prompts and optimizes spending across LLM providers.

**Quality assurance** detects issues and unexpected outputs before they reach users.

**Debugging** enables quick problem identification through complete session examination.

Without proper observability, LLM applications become expensive black boxes that are impossible to systematically improve.

<Frame className="rounded-md">
    <img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/documentation/products/dashboard/overview/dashboard.png" />
</Frame>

## What are LLM usage metrics?

LLM usage metrics provide comprehensive monitoring for your AI applications. Track key indicators like total requests, token usage, errors, latency, and costs. 

Break down analytics by model, user, API key, and prompt for complete visibility into your operations.

---
## Need help?
[Join our discord](https://discord.gg/n6ZPqGP6) â€” we'll help you pick the best fit.