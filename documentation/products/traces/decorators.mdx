---
title: "Decorators"
description: "Use decorators to instrument your code and automatically trace your LLM workflows with minimal code changes."
---
---
## Overview

Decorators provide the simplest way to add comprehensive tracing to your LLM workflows without modifying your existing code structure.  

By adding `@workflow` and `@task` decorators to your functions and classes, you can automatically capture detailed execution traces that show the complete hierarchy of your LLM operations.

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/traces/tracing-example.png" alt="Traces example showing workflow and task hierarchy" />
</Frame>

---
## Compatibility

| Integration | Support | Notes |
| ----------- | ------- | ----- |
| **Keywords AI Native** | ✅ | Built-in tracing with Keywords AI SDK |
| **OpenAI SDK** | ✅ | Python only |
| **Vercel AI SDK** | ❌ | Not currently supported |

---

## Integration

<Card title="Setup" icon="compass" horizontal href="/documentation/products/traces/quickstart">
Make sure if have everything ready before you start.
</Card>

### Implementation

<Tabs>

<Tab title="Keywords AI Native">

#### Annotate your workflows

Use the `@workflow` and `@task` decorators to instrument your code:

<CodeGroup>
```python Python
from keywordsai_tracing.decorators import workflow, task
from keywordsai_tracing.main import KeywordsAITelemetry

k_tl = KeywordsAITelemetry()

@workflow(name="my_workflow")
def my_workflow():
    @task(name="my_task")
    def my_task():
        # Your task logic here
        pass
    my_task()
```
```typescript JavaScript
import { KeywordsAITelemetry } from '@keywordsai/tracing';

const keywordsAi = new KeywordsAITelemetry({
    apiKey: process.env.KEYWORDSAI_API_KEY || "",
    appName: 'my-app'
});

async function myWorkflow() {
    return await keywordsAi.withWorkflow(
        { name: 'my_workflow' },
        async () => {
            const result = await keywordsAi.withTask(
                { name: 'my_task' },
                async () => {
                    // Your task logic here
                    return "task result";
                }
            );
            return result;
        }
    );
}
```
</CodeGroup>

#### Full example with LLM calls

This example demonstrates a complete LLM workflow with three sequential tasks:

1. **Joke Creation** (`joke_creation`): Generates an original joke about OpenTelemetry using GPT-3.5-turbo
2. **Pirate Translation** (`pirate_joke_translation`): Transforms the joke into pirate language 
3. **Signature Generation** (`signature_generation`): Adds a creative signature to the final pirate joke

The `@workflow` decorator wraps the entire process, while each `@task` decorator instruments individual LLM operations.

<CodeGroup>
```python Python
from openai import OpenAI
from keywordsai_tracing.decorators import workflow, task
from keywordsai_tracing.main import KeywordsAITelemetry

k_tl = KeywordsAITelemetry()
client = OpenAI()

@task(name="joke_creation")
def create_joke():
    completion = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": "Tell me a joke about opentelemetry"}],
        temperature=0.5,
        max_tokens=100,
        frequency_penalty=0.5,
        presence_penalty=0.5,
        stop=["\n"],
        logprobs=True,
    )
    return completion.choices[0].message.content

@task(name="signature_generation")
def generate_signature(joke: str):
    completion = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "user", "content": "add a signature to the joke:\n\n" + joke}
        ],
    )
    return completion.choices[0].message.content

@task(name="pirate_joke_translation")
def translate_joke_to_pirate(joke: str):
    completion = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {
                "role": "user",
                "content": "translate the joke to pirate language:\n\n" + joke,
            }
        ],
    )
    return completion.choices[0].message.content

@workflow(name="joke_workflow")
def joke_workflow():
    joke = create_joke()
    pirate_joke = translate_joke_to_pirate(joke)
    signature = generate_signature(pirate_joke)
    return signature

# Run the workflow
result = joke_workflow()
print(result)
```
```typescript JavaScript
import { KeywordsAITelemetry } from '@keywordsai/tracing';
import OpenAI from 'openai';

const keywordsAi = new KeywordsAITelemetry({
    apiKey: process.env.KEYWORDSAI_API_KEY || "",
    appName: 'joke-app'
});

const openai = new OpenAI();

async function createJoke() {
    return await keywordsAi.withTask(
        { name: 'joke_creation' },
        async () => {
            const completion = await openai.chat.completions.create({
                messages: [{ role: 'user', content: 'Tell me a joke about opentelemetry' }],
                model: 'gpt-3.5-turbo',
                temperature: 0.5,
                max_tokens: 100,
                stop: ["\n"]
            });
            return completion.choices[0].message.content;
        }
    );
}

async function generateSignature(joke: string) {
    return await keywordsAi.withTask(
        { name: 'signature_generation' },
        async () => {
            const completion = await openai.chat.completions.create({
                messages: [
                    { role: 'user', content: `add a signature to the joke:\n\n${joke}` }
                ],
                model: 'gpt-3.5-turbo'
            });
            return completion.choices[0].message.content;
        }
    );
}

async function translateJokeToPirate(joke: string) {
    return await keywordsAi.withTask(
        { name: 'pirate_joke_translation' },
        async () => {
            const completion = await openai.chat.completions.create({
                messages: [
                    {
                        role: 'user',
                        content: `translate the joke to pirate language:\n\n${joke}`
                    }
                ],
                model: 'gpt-3.5-turbo'
            });
            return completion.choices[0].message.content;
        }
    );
}

async function jokeWorkflow() {
    return await keywordsAi.withWorkflow(
        { name: 'joke_workflow' },
        async () => {
            const joke = await createJoke();
            const pirateJoke = await translateJokeToPirate(joke);
            const signature = await generateSignature(pirateJoke);
            return signature;
        }
    );
}

// Run the workflow
jokeWorkflow().then(console.log);
```
</CodeGroup>

</Tab>

<Tab title="OpenAI Agents SDK">

#### Annotate your workflows

<CodeGroup>
```python Python
import os
from agents import Agent, Runner, function_tool
from agents.tracing import set_trace_processors
from keywordsai_tracing.integrations.openai_agents_integration import KeywordsAITraceProcessor

set_trace_processors([
    KeywordsAITraceProcessor(
        api_key=os.getenv("KEYWORDSAI_API_KEY"),
        endpoint="https://api.keywordsai.co/api/openai/v1/traces/ingest"
    ),
])

@function_tool
def tell_joke() -> str:
    """Tell a joke about opentelemetry"""
    return "Why did the OpenTelemetry span go to therapy? Because it had too many traces of anxiety!"

joke_agent = Agent(
    name="Joke Teller",
    instructions="You tell jokes using the tell_joke function.",
    tools=[tell_joke],
)

async def simple_joke():
    result = await Runner.run(joke_agent, "Tell me a joke about opentelemetry")
    return result.final_output
```
```typescript JavaScript
import { Agent, BatchTraceProcessor, run, setTraceProcessors } from '@openai/agents';
import { KeywordsAIOpenAIAgentsTracingExporter } from '@keywordsai/exporter-openai-agents';

setTraceProcessors([
  new BatchTraceProcessor(
    new KeywordsAIOpenAIAgentsTracingExporter(),
  ),
]);

const jokeAgent = new Agent({
  name: 'Joke Teller',
  instructions: 'You tell jokes using the tell_joke function.',
  tools: [
    {
      type: 'function',
      function: {
        name: 'tell_joke',
        description: 'Tell a joke about opentelemetry',
        parameters: {
          type: 'object',
          properties: {},
          required: [],
        },
      },
    },
  ],
});

async function simpleJoke() {
  const result = await run(jokeAgent, 'Tell me a joke about opentelemetry');
  return result.finalOutput;
}
```
</CodeGroup>

#### Full example with LLM calls

This example demonstrates a complete LLM workflow with three sequential tasks:

1. **Joke Creation** (`joke_creation`): Generates an original joke about OpenTelemetry using GPT-3.5-turbo
2. **Pirate Translation** (`pirate_joke_translation`): Transforms the joke into pirate language 
3. **Signature Generation** (`signature_generation`): Adds a creative signature to the final pirate joke

<CodeGroup>
```python Python
import asyncio
import os
from agents import Agent, Runner, function_tool
from agents.tracing import set_trace_processors
from keywordsai_tracing.integrations.openai_agents_integration import KeywordsAITraceProcessor

set_trace_processors(
    [
        KeywordsAITraceProcessor(
            api_key=os.getenv("KEYWORDSAI_API_KEY"),
            endpoint="https://api.keywordsai.co/api/openai/v1/traces/ingest"
        ),
    ]
)

# Pirate joke workflow example
@function_tool
def create_joke() -> str:
    """Create a joke about opentelemetry"""
    return "Why did the OpenTelemetry span go to therapy? Because it had too many traces of anxiety!"

@function_tool
def translate_joke_to_pirate(joke: str) -> str:
    """Translate a joke to pirate language"""
    return f"Arrr! {joke.replace('Why', 'Why be').replace('Because', 'Because, ye scurvy dog,')} Yo ho ho!"

@function_tool
def generate_signature(joke: str) -> str:
    """Add a signature to the joke"""
    return f"{joke}\n\n-- Captain Code Beard, Master of the Seven APIs"

joke_creator_agent = Agent(
    name="Joke Creator",
    instructions="You create jokes using the create_joke function.",
    tools=[create_joke],
)

pirate_translator_agent = Agent(
    name="Pirate Translator", 
    instructions="You translate jokes to pirate language using the translate_joke_to_pirate function.",
    tools=[translate_joke_to_pirate],
)

signature_agent = Agent(
    name="Signature Generator",
    instructions="You add signatures to jokes using the generate_signature function.",
    tools=[generate_signature],
)

workflow_agent = Agent(
    name="Joke Workflow Coordinator",
    instructions="Coordinate the joke creation workflow by handing off to appropriate agents in sequence.",
    handoffs=[joke_creator_agent, pirate_translator_agent, signature_agent],
)

async def pirate_joke_workflow():
    # Step 1: Create joke
    joke_result = await Runner.run(joke_creator_agent, "Create a joke about opentelemetry")
    joke = joke_result.final_output
    
    # Step 2: Translate to pirate
    pirate_result = await Runner.run(pirate_translator_agent, f"Translate this joke to pirate language: {joke}")
    pirate_joke = pirate_result.final_output
    
    # Step 3: Add signature
    final_result = await Runner.run(signature_agent, f"Add a signature to this joke: {pirate_joke}")
    
    return final_result.final_output

if __name__ == "__main__":
    result = asyncio.run(pirate_joke_workflow())
    print(result)
```
```typescript JavaScript
import { Agent, BatchTraceProcessor, run, setTraceProcessors, withTrace } from '@openai/agents';
import { KeywordsAIOpenAIAgentsTracingExporter } from '@keywordsai/exporter-openai-agents';

setTraceProcessors([
  new BatchTraceProcessor(
    new KeywordsAIOpenAIAgentsTracingExporter(),
  ),
]);

// Pirate joke workflow example
const createJoke = (): string => {
  return "Why did the OpenTelemetry span go to therapy? Because it had too many traces of anxiety!";
};

const translateJokeToPirate = (joke: string): string => {
  return `Arrr! ${joke.replace('Why', 'Why be').replace('Because', 'Because, ye scurvy dog,')} Yo ho ho!`;
};

const generateSignature = (joke: string): string => {
  return `${joke}\n\n-- Captain Code Beard, Master of the Seven APIs`;
};

const jokeCreatorAgent = new Agent({
  name: 'Joke Creator',
  instructions: 'You create jokes using the create_joke function.',
  tools: [
    {
      type: 'function',
      function: {
        name: 'create_joke',
        description: 'Create a joke about opentelemetry',
        parameters: {
          type: 'object',
          properties: {},
          required: [],
        },
      },
    },
  ],
});

const pirateTranslatorAgent = new Agent({
  name: 'Pirate Translator',
  instructions: 'You translate jokes to pirate language using the translate_joke_to_pirate function.',
  tools: [
    {
      type: 'function',
      function: {
        name: 'translate_joke_to_pirate',
        description: 'Translate a joke to pirate language',
        parameters: {
          type: 'object',
          properties: {
            joke: {
              type: 'string',
              description: 'The joke to translate',
            },
          },
          required: ['joke'],
        },
      },
    },
  ],
});

const signatureAgent = new Agent({
  name: 'Signature Generator',
  instructions: 'You add signatures to jokes using the generate_signature function.',
  tools: [
    {
      type: 'function',
      function: {
        name: 'generate_signature',
        description: 'Add a signature to the joke',
        parameters: {
          type: 'object',
          properties: {
            joke: {
              type: 'string',
              description: 'The joke to add signature to',
            },
          },
          required: ['joke'],
        },
      },
    },
  ],
});

async function pirateJokeWorkflow() {
  return await withTrace('Pirate Joke Workflow', async () => {
    // Step 1: Create joke
    const jokeResult = await run(jokeCreatorAgent, 'Create a joke about opentelemetry');
    const joke = jokeResult.finalOutput;
    
    // Step 2: Translate to pirate
    const pirateResult = await run(pirateTranslatorAgent, `Translate this joke to pirate language: ${joke}`);
    const pirateJoke = pirateResult.finalOutput;
    
    // Step 3: Add signature
    const finalResult = await run(signatureAgent, `Add a signature to this joke: ${pirateJoke}`);
    
    return finalResult.finalOutput;
  });
}

async function main() {
  const result = await pirateJokeWorkflow();
  console.log(result);
}

main().catch(console.error);
```
</CodeGroup>

</Tab>

</Tabs>


### Decorate classes for object-oriented workflows:

<CodeGroup>
```python Python
from openai import OpenAI
from keywordsai_tracing import KeywordsAITelemetry
from keywordsai_tracing.decorators import workflow, task

k_tl = KeywordsAITelemetry()
client = OpenAI()

@workflow(name="joke_agent", method_name="run")
class JokeAgent:
    @task(name="joke_creation")
    def create_joke(self):
        completion = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Tell me a joke about opentelemetry"}],
        )
        joke = completion.choices[0].message.content
        return joke
    
    def run(self):
        return self.create_joke()

# Usage
agent = JokeAgent()
result = agent.run()
```
```typescript JavaScript
// Class-based approach using TypeScript classes
import { KeywordsAITelemetry } from '@keywordsai/tracing';
import OpenAI from 'openai';

const keywordsAi = new KeywordsAITelemetry({
    apiKey: process.env.KEYWORDSAI_API_KEY || "",
    appName: 'joke-app'
});

const openai = new OpenAI();

class JokeAgent {
    async createJoke() {
        return await keywordsAi.withTask(
            { name: 'joke_creation' },
            async () => {
                const completion = await openai.chat.completions.create({
                    messages: [{ role: 'user', content: 'Tell me a joke about opentelemetry' }],
                    model: 'gpt-3.5-turbo'
                });
                return completion.choices[0].message.content;
            }
        );
    }
    
    async run() {
        return await keywordsAi.withWorkflow(
            { name: 'joke_agent' },
            async () => {
                return await this.createJoke();
            }
        );
    }
}

// Usage
const agent = new JokeAgent();
agent.run().then(console.log);
```
</CodeGroup>


## How to see this in the platform

Once you've implemented decorators in your code and executed your workflows, you can view the traces in the Keywords AI platform:

### Accessing traces
1. Navigate to the [Traces page](https://platform.keywordsai.co/platform/traces) in your Keywords AI dashboard
2. You'll see a list of all your traced workflows and tasks

### Understanding the trace view
- **Workflow overview**: See the complete execution flow of your decorated workflows
- **Task breakdown**: View individual tasks within each workflow, including execution time and status
- **LLM call details**: Inspect the actual requests and responses for each LLM operation
- **Performance metrics**: Analyze latency, token usage, and costs for each operation
- **Error tracking**: Identify and debug failures in your workflows

### Example trace visualization
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/traces/tracing-example.png" alt="Traces example showing workflow and task hierarchy" />
</Frame>

The trace view shows:
- The parent workflow (e.g., `joke_workflow`, `vercel_ai_workflow`, `openai_workflow`)
- Individual tasks (e.g., `joke_creation`, `generate_text`, `researcher_agent`)
- Execution timeline and dependencies
- LLM call metadata and performance metrics