---
title: "Model Context Protocol (MCP)"
description: "Access Keywords AI data directly from your AI coding assistant using MCP"
---

The Model Context Protocol (MCP) is a standardized framework that enables AI models to interact with external data sources and tools. It allows for real-time access to your Keywords AI logs, traces, prompts, and customer data directly from your coding environment.

## Getting started

Keywords AI provides an MCP server that gives your AI tools direct access to your observability data. The server supports both HTTP streaming (for hosted deployments) and stdio (for local use).

<CardGroup cols={2}>
  <Card title="Local Stdio" icon="terminal">
    Run locally via command line - simplest setup for personal development
  </Card>
  <Card title="HTTP Streaming" icon="globe">
    Deploy to Vercel for team sharing or remote access
  </Card>
</CardGroup>

## Installation

```bash
git clone https://github.com/Keywords-AI/keywordsai-mcp.git
cd keywordsai-mcp
npm install
npm run build
```

## Usage modes

### Mode 1: Local Stdio (Recommended for personal use)

Run the MCP server locally via stdio - the simplest setup for personal development.

<Steps>
  <Step title="Build the project">
    ```bash
    npm run build
    ```
  </Step>
  <Step title="Configure your AI tool">
    Add to your MCP configuration file:
    
    <CodeGroup>
    ```json Cursor (~/.cursor/mcp.json)
    {
      "mcpServers": {
        "keywords-ai": {
          "command": "node",
          "args": ["/path/to/keywordsai-mcp/dist/lib/index.js"],
          "env": {
            "KEYWORDS_API_KEY": "your_keywords_ai_api_key"
          }
        }
      }
    }
    ```

    ```json Claude Desktop (macOS)
    // ~/Library/Application Support/Claude/claude_desktop_config.json
    {
      "mcpServers": {
        "keywords-ai": {
          "command": "node",
          "args": ["/path/to/keywordsai-mcp/dist/lib/index.js"],
          "env": {
            "KEYWORDS_API_KEY": "your_keywords_ai_api_key"
          }
        }
      }
    }
    ```

    ```json Claude Desktop (Windows)
    // %APPDATA%\Claude\claude_desktop_config.json
    {
      "mcpServers": {
        "keywords-ai": {
          "command": "node",
          "args": ["C:/path/to/keywordsai-mcp/dist/lib/index.js"],
          "env": {
            "KEYWORDS_API_KEY": "your_keywords_ai_api_key"
          }
        }
      }
    }
    ```
    </CodeGroup>
  </Step>
  <Step title="Restart your AI tool">
    Restart Cursor or Claude Desktop to load the MCP server.
  </Step>
</Steps>

---

### Mode 2: HTTP Streaming - Private deployment

Deploy once to Vercel with your API key stored as an environment variable. No client-side key needed.

<Steps>
  <Step title="Deploy to Vercel">
    ```bash
    vercel deploy --prod
    ```
  </Step>
  <Step title="Set environment variable">
    In Vercel Dashboard → Settings → Environment Variables:
    - Add `KEYWORDS_API_KEY` = `your_keywords_ai_api_key`
  </Step>
  <Step title="Configure your AI tool">
    ```json
    {
      "mcpServers": {
        "keywords-ai": {
          "url": "https://your-project.vercel.app/mcp"
        }
      }
    }
    ```
  </Step>
</Steps>

<Note>
This mode is ideal for teams - deploy once, and everyone can use the same URL without exposing API keys in their local config.
</Note>

---

### Mode 3: HTTP Streaming - Public service

Deploy as a shared service where each user provides their own API key via Authorization header.

<Steps>
  <Step title="Deploy to Vercel">
    ```bash
    vercel deploy --prod
    ```
    Do **not** set `KEYWORDS_API_KEY` environment variable.
  </Step>
  <Step title="Configure with Authorization header">
    ```json
    {
      "mcpServers": {
        "keywords-ai": {
          "url": "https://your-project.vercel.app/mcp",
          "headers": {
            "Authorization": "Bearer your_keywords_ai_api_key"
          }
        }
      }
    }
    ```
  </Step>
</Steps>

---

## Authentication

| Mode | API Key Location | Best For |
|------|------------------|----------|
| **Local Stdio** | Environment variable in config | Personal development |
| **Private HTTP** | Vercel environment variable | Team sharing, key hidden |
| **Public HTTP** | Authorization header | Multi-user, each user's own key |

Get your Keywords AI API key from [platform.keywordsai.co](https://platform.keywordsai.co/platform/api/api-keys).

---

## Available tools

Once connected, your AI tools have access to the following Keywords AI features.

### Logs

| Tool | Description |
|------|-------------|
| `list_logs` | List and filter LLM request logs with powerful query capabilities |
| `get_log_detail` | Retrieve complete details of a single log by unique ID |

**Example queries:**
- "Show me the most expensive requests from the last hour"
- "Find all failed requests for customer user_123"
- "List logs where latency > 5 seconds"

### Traces

| Tool | Description |
|------|-------------|
| `list_traces` | List and filter traces with sorting and pagination |
| `get_trace_tree` | Retrieve complete hierarchical span tree of a trace |

**Example queries:**
- "Show me traces with errors in production"
- "Get the span tree for trace xyz123"
- "Find traces that cost more than $0.10"

### Customers

| Tool | Description |
|------|-------------|
| `list_customers` | List customers with pagination and sorting |
| `get_customer_detail` | Get customer details including budget usage |

**Example queries:**
- "Who are my top 10 customers by cost?"
- "Show customer budget usage for user_abc"
- "List customers sorted by number of requests"

### Prompts

| Tool | Description |
|------|-------------|
| `list_prompts` | List all prompts in your organization |
| `get_prompt_detail` | Get detailed prompt information |
| `list_prompt_versions` | List all versions of a prompt |
| `get_prompt_version_detail` | Get specific version details |

**Example queries:**
- "Show me all my prompts"
- "Get the latest version of my customer-support prompt"
- "What are the different versions of prompt xyz?"

---

## Filter reference

All list tools support powerful filtering. Filters are passed as an object where each key is a field name.

### Filter operators

| Operator | Description | Example |
|----------|-------------|---------|
| `""` (empty) | Equal/exact match | `{"operator": "", "value": ["gpt-4"]}` |
| `not` | Not equal | `{"operator": "not", "value": [200]}` |
| `lt` | Less than | `{"operator": "lt", "value": [100]}` |
| `lte` | Less than or equal | `{"operator": "lte", "value": [100]}` |
| `gt` | Greater than | `{"operator": "gt", "value": [0.01]}` |
| `gte` | Greater than or equal | `{"operator": "gte", "value": [0.01]}` |
| `contains` | Contains substring | `{"operator": "contains", "value": ["error"]}` |
| `icontains` | Case insensitive contains | `{"operator": "icontains", "value": ["error"]}` |
| `startswith` | Starts with | `{"operator": "startswith", "value": ["user_"]}` |
| `endswith` | Ends with | `{"operator": "endswith", "value": ["@example.com"]}` |
| `in` | Value in list | `{"operator": "in", "value": ["gpt-4", "gpt-4o"]}` |
| `isnull` | Check if null | `{"operator": "isnull", "value": [true]}` |

### Filter examples

<AccordionGroup>
  <Accordion title="Filter by cost">
    ```json
    {
      "cost": {"operator": "gt", "value": [0.01]}
    }
    ```
  </Accordion>
  <Accordion title="Filter by model">
    ```json
    {
      "model": {"operator": "", "value": ["gpt-4"]}
    }
    ```
  </Accordion>
  <Accordion title="Filter by customer">
    ```json
    {
      "customer_identifier": {"operator": "contains", "value": ["premium"]}
    }
    ```
  </Accordion>
  <Accordion title="Filter by custom metadata">
    ```json
    {
      "metadata__session_id": {"operator": "", "value": ["abc123"]}
    }
    ```
  </Accordion>
  <Accordion title="Multiple filters">
    ```json
    {
      "cost": {"operator": "gte", "value": [0.01]},
      "model": {"operator": "in", "value": ["gpt-4", "gpt-4o"]},
      "status_code": {"operator": "", "value": [200]}
    }
    ```
  </Accordion>
</AccordionGroup>

### Filterable fields

**Logs:**
- Identifiers: `customer_identifier`, `custom_identifier`, `thread_identifier`, `prompt_id`, `unique_id`
- Tracing: `trace_unique_id`, `span_name`, `span_workflow_name`
- Model/Provider: `model`, `deployment_name`, `provider_id`
- Status: `status_code`, `status`, `error_message`, `failed`
- Metrics: `cost`, `latency`, `tokens_per_second`, `time_to_first_token`, `prompt_tokens`, `completion_tokens`
- Config: `environment`, `log_type`, `stream`, `temperature`, `max_tokens`
- Custom: `metadata__your_field` (prefix with `metadata__`)

**Traces:**
- `trace_unique_id`, `customer_identifier`, `environment`
- `span_count`, `llm_call_count`, `error_count`
- `total_cost`, `total_tokens`, `duration`
- Custom: `metadata__your_field`

---

## Example conversations

### Debugging high-cost requests

> **You:** "Show me the most expensive API calls from the last 24 hours"
>
> **AI:** Uses `list_logs` with `sort_by: "-cost"` and `start_time` filter to find expensive requests, then analyzes the results.

### Investigating errors

> **You:** "Find all failed requests for customer premium_user_42"
>
> **AI:** Uses `list_logs` with filters `{"failed": {"operator": "", "value": [true]}, "customer_identifier": {"operator": "", "value": ["premium_user_42"]}}`

### Analyzing trace performance

> **You:** "Show me the slowest traces in production this week"
>
> **AI:** Uses `list_traces` with `environment: "production"`, `sort_by: "-duration"`, and appropriate time range.

### Reviewing prompts

> **You:** "What prompts do we have and show me the latest version of the support-agent prompt"
>
> **AI:** Uses `list_prompts` to show all prompts, then `get_prompt_detail` for the specific prompt.

---

## Troubleshooting

<AccordionGroup>
  <Accordion title="MCP server not appearing in Cursor/Claude">
    - Verify your JSON configuration syntax is valid
    - Check the file path to `index.js` is correct (use absolute paths)
    - Restart your AI tool completely
    - Check that `npm run build` completed successfully
  </Accordion>
  
  <Accordion title="Authentication errors">
    - Verify your API key is correct at [platform.keywordsai.co](https://platform.keywordsai.co/platform/api/api-keys)
    - For stdio mode: Check the `KEYWORDS_API_KEY` is set in your config's `env` section
    - For HTTP mode: Verify the Authorization header format is `Bearer your_key`
  </Accordion>
  
  <Accordion title="Connection timeouts (HTTP mode)">
    - Check your internet connection
    - Verify the Vercel deployment URL is correct
    - Ensure the Vercel function hasn't exceeded its timeout limit
  </Accordion>
  
  <Accordion title="Empty results from queries">
    - Check your time range - default is last 1 hour
    - Verify you have data in the specified environment (prod vs test)
    - Try removing filters to see if any data exists
  </Accordion>
  
  <Accordion title="Build errors on Vercel">
    - Clear the build cache in Vercel Dashboard and redeploy
    - Verify all dependencies are in `package.json`
    - Check the build logs for specific error messages
  </Accordion>
</AccordionGroup>

---

## Source code

The Keywords AI MCP server is open source:

<Card title="GitHub Repository" icon="github" href="https://github.com/Keywords-AI/keywordsai-mcp">
  View source code, report issues, and contribute
</Card>
