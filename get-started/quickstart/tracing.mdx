---
title: "Agent tracing"
description: "You can use Keywords AI Traces to trace your LLM requests and responses."
og:title: 'LLM tracing'
---

## What is traces?
Traces are a chained collection of workflows and tasks. You can use tree views and waterfalls to better track dependencies and latency.

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/observability/overview/traces.png" alt="Traces example" />
</Frame>

## Integrate with your existing AI framework

Keywords AI integrates seamlessly with popular AI frameworks to give you complete observability into your agent workflows.

## Supported frameworks


<CardGroup cols={2}>
<Card href="/integration/development-frameworks/tracing/openai-agents-sdk" className="bg-white">
  <img className="block dark:hidden" src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/Integrations/integration_cards/openai_agent_v0.png" alt="OpenAI Agents SDK" style={{
    pointerEvents: 'none',
  }} />
  <img className="hidden dark:block" src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/Integrations/integration_cards/openai_agent_v0_black.png" alt="OpenAI Agents SDK" style={{
    pointerEvents: 'none',
  }} />
</Card>
<Card href="/integration/development-frameworks/tracing/vercel-tracing">
  <img className="block dark:hidden" src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/Integrations/integration_cards/vercel_v0.png" alt="Vercel AI SDK" style={{
    pointerEvents: 'none'
  }} />
  <img className="hidden dark:block" src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/Integrations/integration_cards/vercel_v0_black.png" alt="Vercel AI SDK" style={{
    pointerEvents: 'none'
  }} />
</Card>
<Card href="/integration/development-frameworks/tracing/mastra">
  <img className="block dark:hidden" src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/Integrations/integration_cards/mastra_v0.png" alt="Mastra" style={{
    pointerEvents: 'none'
  }} />
  <img className="hidden dark:block" src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/Integrations/integration_cards/mastra_v0_black.png" alt="Mastra" style={{
    pointerEvents: 'none'
  }} />
</Card>
</CardGroup>

## Keywords AI native tracing
If you don't want to choose any AI framework right now, you can also use Keywords AI native tracing for your AI agents without any additional setup.  

You just need to add the `keywordsai-tracing` package to your project and annotate your workflows.
 

<Tabs>
<Tab title="Python">

<Steps>
<Step title="Install the SDK">
Install the package using your preferred package manager:
<CodeGroup>
```bash pip
pip install keywordsai-tracing
```
```bash poetry
poetry add keywordsai-tracing
```
</CodeGroup>
</Step>

<Step title="Set up Environment Variables">
Get your API key from the [API Keys page](https://platform.keywordsai.co/platform/api/api-keys) in Settings, then configure it in your environment:

```python .env
KEYWORDSAI_BASE_URL="https://api.keywordsai.co/api"
KEYWORDSAI_API_KEY="YOUR_KEYWORDSAI_API_KEY"
```
</Step>

<Step title="Annotate your workflows">
Use the `@workflow` and `@task` decorators to instrument your code:
```python Python {6, 8}
from keywordsai_tracing.decorators import workflow, task
from keywordsai_tracing.main import KeywordsAITelemetry

k_tl = KeywordsAITelemetry()

@workflow(name="my_workflow")
def my_workflow():
    @task(name="my_task")
    def my_task():
        pass
    my_task()
```
</Step>

<Step title="A full example with LLM calls">
In this example, you will see how to implement a workflow that includes LLM calls. We use OpenAI SDK as an example.

```python main.py {2-3, 5, 8}
from openai import OpenAI
from keywordsai_tracing.decorators import workflow, task
from keywordsai_tracing.main import KeywordsAITelemetry

k_tl = KeywordsAITelemetry()
client = OpenAI()

@workflow(name="create_joke")
def create_joke():
    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": "Tell me a joke about opentelemetry"}],
        temperature=0.5,
        max_tokens=100,
        frequency_penalty=0.5,
        presence_penalty=0.5,
    )
    return completion.choices[0].message.content
```
</Step>

</Steps>
</Tab>

<Tab title="JS/TS">
<Steps>
<Step title="Install the SDK">
Install the package using your preferred package manager:
```bash 
npm install @keywordsai/tracing
# or yarn

yarn add @keywordsai/tracing
```
</Step>
<Step title="Set up Environment Variables">
Get your API key from the [API Keys page](https://platform.keywordsai.co/platform/api/api-keys) in Settings, then configure it in your environment:

```bash .env
KEYWORDSAI_BASE_URL="https://api.keywordsai.co/api"
KEYWORDSAI_API_KEY="YOUR_KEYWORDSAI_API_KEY"
```
</Step>
<Step title="Create a simple task">
```typescript server.ts {1, 6-10, 16-17}
import { KeywordsAITelemetry } from '@keywordsai/tracing';
import OpenAI from 'openai';

// Initialize clients
// Make sure to set these environment variables or pass them directly
const keywordsAi = new KeywordsAITelemetry({
    apiKey: process.env.KEYWORDSAI_API_KEY || "",
    appName: 'test-app',
    disableBatch: true  // For testing, disable batching
});

const openai = new OpenAI();

// This demonstrates a simple LLM call wrapped in a task
async function createJoke() {
    return await keywordsAi.withTask(
        { name: 'joke_creation' },
        async () => {
            const completion = await openai.chat.completions.create({
                messages: [{ role: 'user', content: 'Tell me a joke about TypeScript' }],
                model: 'gpt-4o-mini',
                temperature: 0.7
            });
            return completion.choices[0].message.content;
        }
    );
}
```
</Step>

<Step title="Create a workflow combining tasks">
In this example, we create a workflow `pirate_joke_workflow` that combines the `createJoke` task with a `translateJoke` task.
```typescript server.ts {2-3, 5-6}
async function jokeWorkflow() {
    return await keywordsAi.withWorkflow(
        { name: 'pirate_joke_workflow' },
        async () => {
            const joke = await createJoke();
            const pirateJoke = await translateJoke(joke);
            return pirateJoke;
        }
    );
}
```
</Step>

</Steps>
</Tab>

</Tabs>

You can now see your traces in the [Traces](https://platform.keywordsai.co/platform/traces) and go to [Logs](https://platform.keywordsai.co/platform/requests) to see the details of your LLM calls.