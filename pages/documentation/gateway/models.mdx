---
title: "Supported models"
description: "Call 200+ LLMs with a single OpenAI compatible format"
---

The most used feature of the LLM proxy is the ability to call 200+ LLMs with a single API format. You can switch between models with only 1 line of code change. 

## Choose a model
After you integrate the LLM proxy, you can choose a model from the [Models page](https://platform.keywordsai.co/platform/models). In this page, you can see each model's description, pricing, and other metrics, which helps you choose the best model for your use case.
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/llm-proxy/models.png"/>
</Frame>

### Model family
You can also click an exact model to see it's model family, which is a group of models that hosted by the different LLM providers.
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/llm-proxy/model-details.png"/>
</Frame>

### Integration code
If you have already integrated the LLM proxy, you can click the `Code` button to copy the integration code with the language you are using.
<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/llm-proxy/model-code.png"/>
</Frame>

## Call models in different frameworks
<Tabs>
<Tab title="OpenAI Python SDK">
```python {9}
from openai import OpenAI

client = OpenAI(
    base_url="https://api.keywordsai.co/api/",
    api_key="YOUR_KEYWORDSAI_API_KEY",
)

response = client.chat.completions.create(
    model="claude-3-5-haiku-20241022",
    messages=[
        {"role": "user", "content": "Tell me a long story"}
    ]
)
```
</Tab>
<Tab title="OpenAI TypeScript SDK">
Here is an example of how to disable logging in the OpenAI TypeScript SDK. In OpenAI TypeScript SDK, you should add a `// @ts-expect-error` before the disable_log field.
```typescript {11}
import { OpenAI } from "openai";

const client = new OpenAI({
  baseURL: "https://api.keywordsai.co/api",
  apiKey: "YOUR_KEYWORDSAI_API_KEY",
});

const response = await client.chat.completions
  .create({
    messages: [{ role: "user", content: "Say this is a test" }],
    model: "claude-3-5-sonnet-20241022"
  })
  .asResponse();

console.log(await response.json());
```
</Tab>
<Tab title="Standard API">
```python  {3}
import requests
def demo_call(input, 
              model="groq/llama-3.3-70b-specdec",
              token="YOUR_KEYWORDS_AI_API_KEY",
              ):
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {token}',
    }

    data = {
        'model': model,
        'messages': [{'role': 'user', 'content': input}],
    }

    response = requests.post('https://api.keywordsai.co/api/chat/completions', headers=headers, json=data)
    return response

messages = "Say 'Hello World'"
print(demo_call(messages).json())
```
</Tab>
<Tab title="Other SDKs">
We also support adding credentials in other SDKs or languages, please check out our [integration section](/integration/own-api-keys) for more information.
</Tab>
</Tabs>

{/* ## Response

The results should be printed in the console.

```json
{
  "id": "chatcmpl-8Ygj0WAGNhHBFjatPCefcPeNi12ct",
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "message": { "content": "Hello World", "role": "assistant" }
    }
  ],
  "created": 1703230636,
  "model": "gpt-3.5-turbo",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": { "completion_tokens": 2, "prompt_tokens": 12, "total_tokens": 14 },
  "_response_ms": 653.2679999999999
}
``` */}


